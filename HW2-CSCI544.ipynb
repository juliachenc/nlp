{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2-CSCI544.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyOPPoXQD2UeRLM4jkPRETJi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JI7c1QX3R9Au","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633375735263,"user_tz":420,"elapsed":4723,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"63f53afb-35b9-4a8d-9163-d7d5b7d6a76c"},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","import re\n","from bs4 import BeautifulSoup\n","import gensim.models\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.optim as optim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"ClYs2F6q7kw1"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaXd2zZvH_ns","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633374923878,"user_tz":420,"elapsed":3806,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"cf772ab6-d9bf-4eff-c347-3e7d2abfbd68"},"source":["! pip install contractions"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"53TACNobIOMw"},"source":["## 1. Dataset Generation"]},{"cell_type":"markdown","metadata":{"id":"wxp9224t357a"},"source":["In this section, we will load the data, keep the impoartant features and remove the missing values. \n","\n","We assume there 3 different sentiments: \n","*   rating star less than 3 denote negative (class 2) labeled as 0\n","*   rating star more than 3 denote positive (class 1) labeled as 1\n","*   rating star equals 3 denote neutral sentiment\n","\n","We will spilt the data later after we perform data cleaning and pre-processing later in section 3.\n"]},{"cell_type":"code","metadata":{"id":"1YC6GlHyICxw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633375736482,"user_tz":420,"elapsed":198,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"0137d7fa-9ef0-4d4c-95d8-1c5e40166047"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"HolOCYcvIbkh"},"source":["df = pd.read_csv(\"/content/drive/MyDrive/amazon_reviews_us_Kitchen_v1_00.tsv\", \n","                 sep='\\t',\n","                 #usecols = ['star_rating','review_body'],df\n","                 error_bad_lines=False,\n","                 warn_bad_lines=False)\n","np.random.seed(2021)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRd6kfvwEdn2"},"source":["# Keep Rviews and Ratings\n","df = df[[\"star_rating\",\"review_body\"]]\n","\n","# Drop na \n","df_withoutmissing = df.dropna()\n","\n","# select 250k,  50k for each star_rating group \n","data = df_withoutmissing.groupby('star_rating').apply(lambda x:\n","                                                      x.sample(50000, random_state = 100)).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpyybpO2IpP2"},"source":["# Labelling Reviews\n","review_df = data.copy()\n","review_df.loc[3, 'sentiment'] = None\n","\n","row_idx_0 = review_df[review_df['star_rating']<=2].index\n","row_idx_1 = review_df[review_df['star_rating']>=4].index\n","row_idx_2 = review_df[review_df['star_rating']==3].index\n","\n","review_df.loc[row_idx_0,'sentiment'] = 0\n","review_df.loc[row_idx_1,'sentiment'] = 1\n","review_df.loc[row_idx_2,'sentiment'] = 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRhNoML3g-3v"},"source":["## 2. Word Embedding"]},{"cell_type":"markdown","metadata":{"id":"R1HotFQjo52L"},"source":["### (a) Load Pretrained word2wec Model"]},{"cell_type":"code","metadata":{"id":"6Uvd_KnzpAlT"},"source":["import gensim.downloader as api\n","path = api.load(\"word2vec-google-news-300\", return_path=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9pcclZTpqWR"},"source":["from gensim.models import KeyedVectors\n","pretrained_model = KeyedVectors.load_word2vec_format(path, binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L59oro3zsGkA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633375109965,"user_tz":420,"elapsed":29,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"bbc21c31-7816-499a-83a8-46b818290409"},"source":["# example 1\n","w1=\"dirty\"\n","w2=\"smelly\"\n","print(\"The similarity between\", w1, \"and\", w2, \"on pretrained model is: \", pretrained_model.wv.similarity(w1, w2))\n","\n","# example 2 \n","w3=\"dog\"\n","w4=\"puppy\"\n","print(\"The similarity between\", w3, \"and\", w4, \"on pretrained model is: \", pretrained_model.wv.similarity(w3, w4))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The similarity between dirty and smelly on pretrained model is:  0.57099473\n","The similarity between dog and puppy on pretrained model is:  0.81064284\n"]}]},{"cell_type":"markdown","metadata":{"id":"CrrJe3wnsRdu"},"source":["### (b) Train Word2Vec model using own dataset."]},{"cell_type":"code","metadata":{"id":"-zq1is8Itar9"},"source":["documents = []\n","for d in review_df.review_body:\n","    documents.append(gensim.models.utils.simple_preprocess(d))\n","# documents = [row.lower().split() for row in review_df['review_body']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMmcxAUv992z"},"source":["from gensim.models import Word2Vec\n","\n","w2v_model = Word2Vec(documents, size=300, window=11, min_count=10)\n","#w2v_model.build_vocab(documents)\n","#w2v_model.train(documents, total_examples=len(documents), epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXiHXwcHjJAZ"},"source":["\"\"\"run here\"\"\"\n","#path = '/content/drive/MyDrive/DSCI 544/hw2/w2v_model'\n","#w2v_model.save(path)\n","w2v_model = gensim.models.Word2Vec.load('/content/drive/MyDrive/DSCI 544/hw2/w2v_model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uecJkIpttklB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633375297074,"user_tz":420,"elapsed":21,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"bf5ca9da-842b-4dea-abbd-829346dd4267"},"source":["# example 1\n","print(\"The similarity between\", w1, \"and\", w2, \"on my word2vec model is: \", w2v_model.wv.similarity(w1, w2))\n","\n","# example 2 \n","print(\"The similarity between\", w3, \"and\", w4, \"on my word2vec model is: \", w2v_model.wv.similarity(w3, w4))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The similarity between dirty and smelly on my word2vec model is:  0.4769745\n","The similarity between dog and puppy on my word2vec model is:  0.2914985\n"]}]},{"cell_type":"markdown","metadata":{"id":"pkjuRV617vnI"},"source":["From Section 2 part (a), we get that the similarity between **dirty** and **smelly**, and the similarity between **dog** and **puppy** are higher than what we get on our own word2vec model.  Hence we can conclude that the pretrained model encoding semantic similarities between words better. \n","\n","I think the main reason it is happening is that the pretrained model has been trained on a ton of data and encodes the contextual/semantic similarities between words."]},{"cell_type":"markdown","metadata":{"id":"PmOQtZEutyh0"},"source":["## 3. Simple models"]},{"cell_type":"markdown","metadata":{"id":"T3Z_nncx9iy_"},"source":["In this section, we will perform data cleaning and preprocessing, which is the same as HW1, to `review_df`. \n","For simple models, we will only use positive and negative sentiment data. Ealier we have assigned positive (class 1) as label 1 and negative (class 2) as label 0. We know have 250k data which also include neutral sentiment named `review_df`, and 200k data which only have postive and negative named  `samples`.\n","\n","After cleaning, we will split the data into 80% training and 20% testing for modeling on 200k data."]},{"cell_type":"markdown","metadata":{"id":"MSCG8lmei-y2"},"source":["### Data cleaning\n","1. convert to lower case\n","2. remove html and url\n","3. perform contractions\n","4. remove non-alphabetical characters\n","5. remove the extra spaces between the words\n"]},{"cell_type":"code","metadata":{"id":"1zUE-04ZUW5v"},"source":["##### lower case \n","review_df.loc[:, \"review_body\"] = review_df.loc[:, \"review_body\"].str.lower()\n","\n","##### remove html and url\n","def remove_url(text):\n","    url=re.compile(r\"https?://\\S+|www\\.\\S+\")\n","    return url.sub(r\" \",text)\n","def remove_html(text):\n","    cleanr = re.compile('<.*?>')\n","    return cleanr.sub(r\" \",text)\n","    \n","review_df[\"review_body\"] = review_df[\"review_body\"].map(lambda x:remove_url(x))\n","review_df[\"review_body\"] = review_df[\"review_body\"].map(lambda x:remove_html(x))\n","\n","##### perform contractions\n","import contractions\n","review_df[\"review_body\"] = review_df[\"review_body\"].apply(\n","    lambda x: ' '.join(contractions.fix(word) for word in x.split()))\n","\n","##### remove non-alphabetical characters\n","regex = '[^a-zA-Z]'\n","review_df[\"review_body\"] = review_df[\"review_body\"].replace(regex, ' ', regex=True)\n","\n","##### remove the extra spaces between the words\n","review_df[\"review_body\"] = review_df[\"review_body\"].replace('\\s+', ' ', regex=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LW40L6Drjb2i"},"source":["### Data pre-processing\n","1. remove stop word\n","2. perform lemmatization\n","\n"]},{"cell_type":"code","metadata":{"id":"MmlnHgiTja-1"},"source":["##### remove stop word \n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","\n","review_df[\"review_body\"] = review_df[\"review_body\"].apply(\n","    lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n","\n","##### perform lemmatization\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","review_df[\"review_body\"] = review_df[\"review_body\"].apply(\n","    lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMaZxKpL_2XH"},"source":["Select only postive and negative reviews."]},{"cell_type":"code","metadata":{"id":"TphtmQyELmJb"},"source":["# keep postive (class 1) and negative rating (class 2) from data\n","#positive = review_df[review_df['sentiment']==1]\n","#negative = review_df[review_df['sentiment']==0]\n","samples = pd.concat([review_df[review_df['sentiment']==1], review_df[review_df['sentiment']==0]])\n","samples = samples.reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Na51rBnkQCoS"},"source":["# split data into 80% training dataset and 20% testing dataset.\n","from sklearn.model_selection import train_test_split\n","training_review, testing_review = train_test_split(samples, test_size=.2, random_state=42)\n","training_review_250k, testing_review_250k = train_test_split(review_df, test_size=.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kuuzhEXArn0e"},"source":["# convert reviews into list \n","training_X = training_review[\"review_body\"].to_list() \n","training_y = training_review[\"sentiment\"].values\n","\n","testing_X = testing_review[\"review_body\"].to_list() \n","testing_y = testing_review[\"sentiment\"].values\n","\n","training_X_250k = training_review_250k[\"review_body\"].to_list() \n","training_y_250k = training_review_250k[\"sentiment\"].values\n","\n","testing_X_250k = testing_review_250k[\"review_body\"].to_list() \n","testing_y_250k = testing_review_250k[\"sentiment\"].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W4la2GekXgK0"},"source":["Get the mean Word2Vec for each review whihc is saying that each word corresponding to one vector, one sentence eqauls the average of vectors (words)\n"]},{"cell_type":"code","metadata":{"id":"JasmbkGtkB9_"},"source":["\"\"\"def get_mean_vector(review):\n","    output = []\n","    for x in review:\n","        if x not in w2v_model.wv.vocab:\n","            output.append(np.zeros(300))\n","        else:\n","            output.append(w2v_model[x])\n","    return np.array(output).mean(axis=0) \"\"\"\n","\n","def get_mean_vector(model, words):\n","    word = np.zeros(300)\n","    count = 0 # count of valid word\n","    for i in words.split():    \n","        if i in model:\n","            word += model[i]\n","            count += 1\n","    if count == 0:\n","        return word\n","    if count > 0:\n","        return word/count\n","        # np.mean(word, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"igWLERLquWtJ"},"source":["### My word2vec model "]},{"cell_type":"code","metadata":{"id":"pBrvZL69tDDU"},"source":["# apply mean vector to 200k data\n","my_x_train  = []\n","my_x_test = []\n","for x in training_X:\n","    my_x_train.append(get_mean_vector(w2v_model, x))\n","for x in testing_X:\n","    my_x_test.append(get_mean_vector(w2v_model, x))\n","\n","# apply mean vector to 250k data\n","my_x_train_250k  = []\n","my_x_test_250k = []\n","for x in training_X_250k:\n","    my_x_train_250k.append(get_mean_vector(w2v_model, x))\n","for x in testing_X_250k:\n","    my_x_test_250k.append(get_mean_vector(w2v_model, x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2urJvigy-n1"},"source":["### Google word2vec model"]},{"cell_type":"code","metadata":{"id":"QMdjmDoczDP6"},"source":["# apply mean vector to 200k data\n","google_x_train  = []\n","google_x_test = []\n","for x in training_X:\n","    google_x_train.append(get_mean_vector(pretrained_model, x))\n","for x in testing_X:\n","    google_x_test.append(get_mean_vector(pretrained_model, x))\n","\n","# apply mean vector to 200k data\n","google_x_train_250k  = []\n","google_x_test_250k = []\n","for x in training_X_250k:\n","    google_x_train_250k.append(get_mean_vector(pretrained_model, x))\n","for x in testing_X_250k:\n","    google_x_test_250k.append(get_mean_vector(pretrained_model, x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hb2H1tiFLm6l"},"source":["### (a) Perceptron: Model 1 & Model 2"]},{"cell_type":"code","metadata":{"id":"hKHCThI1YqH3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633375685084,"user_tz":420,"elapsed":2954,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"9012ab6b-0251-47f7-faac-4113c78ec801"},"source":["from sklearn.linear_model import Perceptron\n","\n","# fit the training data on google word2vec model\n","Percet1= Perceptron()\n","Percet1.fit(google_x_train, training_y)\n","\n","# fit the training data on my word2vec model\n","Percet2= Perceptron()\n","Percet2.fit(my_x_train, training_y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n","           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n","           penalty=None, random_state=0, shuffle=True, tol=0.001,\n","           validation_fraction=0.1, verbose=0, warm_start=False)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"lEQRypmP9dhw"},"source":["predictions_Percet_train_google = Percet1.predict(google_x_train)\n","predictions_Percet_test_google = Percet1.predict(google_x_test)\n","\n","predictions_Percet_train = Percet2.predict(my_x_train)\n","predictions_Percet_test = Percet2.predict(my_x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OfA3X_VwsMb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633375686281,"user_tz":420,"elapsed":16,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"57ca20f4-51b4-4655-fe32-a451903eeb5d"},"source":["print(\"My Word2Vec model Perceptron Training Accuracy Score:\", accuracy_score(predictions_Percet_train_google, training_y))\n","print(\"My Word2Vec model Perceptron Testing Accuracy Score::\", accuracy_score(predictions_Percet_test_google, testing_y))\n","\n","print(\"Pretrained model Perceptron Training Accuracy Score:\", accuracy_score(predictions_Percet_train, training_y))\n","print(\"Pretrained model Perceptron Testing Accuracy Score:\", accuracy_score(predictions_Percet_test, testing_y))\n","\n","print(\"TF-IDF model Perceptron Training Accuracy Score: 0.99358125\")\n","print(\"TF-IDF model Perceptron Testing Accuracy Score: 0.89785\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["My Word2Vec model Perceptron Training Accuracy Score: 0.72443125\n","My Word2Vec model Perceptron Testing Accuracy Score:: 0.7204\n","Pretrained model Perceptron Training Accuracy Score: 0.7895875\n","Pretrained model Perceptron Testing Accuracy Score: 0.787475\n","TF-IDF model Perceptron Training Accuracy Score: 0.99358125\n","TF-IDF model Perceptron Testing Accuracy Score: 0.89785\n"]}]},{"cell_type":"markdown","metadata":{"id":"7vZGlCh6L2wJ"},"source":["### (b) SVM: Model 3 & Model 4"]},{"cell_type":"code","metadata":{"id":"GyG0qz-Ftx9V"},"source":["from sklearn import svm\n","from sklearn.svm import LinearSVC\n","\n","# fit the training data on google model\n","SVM1 = svm.LinearSVC()\n","SVM1.fit(google_x_train, training_y)\n","\n","# fit the training data on my word2vec model\n","SVM2 = svm.LinearSVC()\n","SVM2.fit(my_x_train, training_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzChMb77BolA"},"source":["predictions_SVM_train_google = SVM1.predict(google_x_train)\n","predictions_SVM_test_google = SVM1.predict(google_x_test)\n","\n","predictions_SVM_train = SVM2.predict(my_x_train)\n","predictions_SVM_test = SVM2.predict(my_x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4dEY7BV1rZb"},"source":["import sys\n","np.set_printoptions(threshold=sys.maxsize)\n","# predictions_SVM_test_google"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ar61qfStB5DQ"},"source":["print(\"My Word2Vec model SVM Training Accuracy Score:\", accuracy_score(predictions_SVM_train_google, training_y))\n","print(\"My Word2Vec model SVM Testing Accuracy Score\", accuracy_score(predictions_SVM_test_google, testing_y))\n","\n","print(\"Pretrained model SVM Training Accuracy Score:\", accuracy_score(predictions_SVM_train, training_y))\n","print(\"Pretrained model SVM Testing Accuracy Score:\", accuracy_score(predictions_SVM_test, testing_y))\n","\n","print(\"TF-IDF model SVM Training Accuracy Score: 0.9949125\")\n","print(\"TF-IDF model SVM Testing Accuracy Score: 0.91585\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gsdpzut1veY4"},"source":["From this question, we get that the accuracy scores:\n","\n","On Perceptron: \n","*  word2vec-google-news-300: **0.7204**\n","*  self trained Word2Vec: **0.787475**\n","*  TF-IDF model: **0.89785**\n","\n","On SVM\n","*  word2vec-google-news-300: **0.845325**\n","*  self trained Word2Vec: **0.816**\n","*  TF-IDF model: **0.91585**\n","\n","I also reported the training accuracy with testing accuracy, and we may see that the testing scores are a bit lower than training which tells us that our model is not overfitting. Besides, we also can observe that the SVM model relatively gives higher scores than perceptron. "]},{"cell_type":"code","metadata":{"id":"wwHpr9xbWTtA"},"source":["del my_x_train\n","del my_x_test\n","del my_x_test_250k\n","del my_x_train_250k\n","\n","del google_x_test\n","del google_x_train\n","del google_x_test_250k\n","del google_x_train_250k\n","\n","del predictions_Percet_train_google\n","del predictions_SVM_test_google\n","del predictions_Percet_train\n","del predictions_Percet_test\n","\n","del predictions_SVM_train_google\n","del predictions_SVM_test_google\n","del predictions_SVM_train\n","del predictions_SVM_test\n","\n","del Percet1\n","del Percet2\n","del SVM1\n","del SVM2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uzKptKsxB1Wq"},"source":["## 4. Feedforward Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"PSnYTjwDDBZy"},"source":["### (a) Part 1: FF Network for Binary Classiffication"]},{"cell_type":"code","metadata":{"id":"rd8jmI9Q5KUe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633376089109,"user_tz":420,"elapsed":155,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"5fcf1c67-be17-48a2-cf5f-64609746d69c"},"source":["is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")\n","    \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available\n"]}]},{"cell_type":"code","metadata":{"id":"E5fHeX7ImZkI"},"source":["# create Tensor datasets\n","#train_data = TensorDataset(torch.from_numpy(np.asarray(rnn_my_x_train)).float(), torch.from_numpy(training_y).float())\n","#test_data = TensorDataset(torch.from_numpy(np.asarray(rnn_my_x_test)).float(), torch.from_numpy(testing_y).float())\n","training_y_long=torch.from_numpy(training_y)\n","training_y_long=torch.tensor(training_y, dtype=torch.long)\n","\n","testing_y_long=torch.from_numpy(testing_y)\n","testing_y_long=torch.tensor(testing_y, dtype=torch.long)\n","\n","train_dat_long = list(zip(training_X,training_y_long))\n","test_dat_long = list(zip(testing_X,testing_y_long))\n","\n","# dataloaders\n","batch_size = 200\n","\n","# make sure to shuffle data\n","train_loader_long = DataLoader(train_dat_long, shuffle=True, batch_size=batch_size)\n","test_loader_long = DataLoader(test_dat_long, shuffle=True, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58bzAwRSo-fR"},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","def categorical_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    top_pred = preds.argmax(1, keepdim = True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","\n","def train(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch_id, (text, y) in enumerate(iterator):\n","        optimizer.zero_grad()\n","        predictions = model(text)#.squeeze(1)\n","        #print(predictions)\n","        \n","        loss = criterion(predictions, y.to(device))\n","        acc = categorical_accuracy(predictions, y.to(device))\n","        \n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        #print(f'Batch ID: {batch_id}/{len(iterator)}, Training Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch_id, (text, y) in enumerate(iterator):\n","\n","            predictions = model(text)#.squeeze(1)\n","            loss = criterion(predictions, y.to(device))\n","            acc = categorical_accuracy(predictions, y.to(device))\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            #print(f'Batch ID: {batch_id}/{len(iterator)}, Test Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyTfYx6PLLDN"},"source":["#### Model 1: Using google pretrained model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDq0tEFRmVn9","executionInfo":{"status":"ok","timestamp":1633376124937,"user_tz":420,"elapsed":267,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"8e762ba5-ed87-4af3-a45f-739e43ca2785"},"source":["class binary_FNN1(nn.Module):\n","    def __init__(self):\n","        super(binary_FNN1, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(300, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 2)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(get_mean_vector(pretrained_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        #embedded.to(device)\n","        #print(embedded.device )\n","\n","        #embedded = [batch size, emb dim]\n","        #print(embedded.shape)\n","        # add hidden layer, with relu activation function\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.fc3(x)\n","        #x = F.tanh(self.fc2(x))  # tanh activation\n","        #print(x.shape)\n","        return x\n","        \n","binary_FNN_model1= binary_FNN1()\n","binary_FNN_model1.to(device)\n","print(binary_FNN_model1)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(binary_FNN_model1.parameters(), lr=0.01)\n","optimizer2 = torch.optim.Adam(binary_FNN_model1.parameters(), lr=0.02)\n","optimizer3 = torch.optim.Adam(binary_FNN_model1.parameters(), lr=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["binary_FNN1(\n","  (fc1): Linear(in_features=300, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4iRLFTHpAQJ","executionInfo":{"status":"ok","timestamp":1633376191957,"user_tz":420,"elapsed":64465,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"042dd0b6-3ff3-4650-d34a-9bb91b99c5c0"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(binary_FNN_model1, train_loader_long, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(binary_FNN_model1, test_loader_long, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 31s\n","\tTrain Loss: 0.413 | Train Acc: 81.19%\n","\tTest Loss: 0.397 |  Test Acc: 82.50%\n","Epoch: 02 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.384 | Train Acc: 82.83%\n","\tTest Loss: 0.383 |  Test Acc: 82.99%\n"]}]},{"cell_type":"markdown","metadata":{"id":"rn925VAsvK3T"},"source":["#### Model 2: Using my word2vec model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGdEY4Rt09tN","executionInfo":{"status":"ok","timestamp":1633376295702,"user_tz":420,"elapsed":143,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"3646d83c-5817-4ab1-d8bd-edb869b0ce6d"},"source":["class binary_FNN2(nn.Module):\n","    def __init__(self):\n","        super(binary_FNN2, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(300, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 2)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(get_mean_vector(w2v_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        #embedded = [batch size, emb dim]\n","        #print(embedded.shape)\n","        # add hidden layer, with relu activation function\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.fc3(x)\n","        #x = F.tanh(self.fc2(x))  # tanh activation\n","        #print(x.shape)\n","        return x \n","        \n","binary_FNN_model2 = binary_FNN2()\n","binary_FNN_model2.to(device)\n","print(binary_FNN_model2)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(binary_FNN_model2.parameters(), lr=0.01)\n","optimizer2 = torch.optim.Adam(binary_FNN_model2.parameters(), lr=0.02)\n","optimizer3 = torch.optim.Adam(binary_FNN_model2.parameters(), lr=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["binary_FNN2(\n","  (fc1): Linear(in_features=300, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pw7KyuUh1FyE","executionInfo":{"status":"ok","timestamp":1633376466316,"user_tz":420,"elapsed":166507,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"eb853a4a-86d7-4b73-9057-814b1ce58a1c"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(binary_FNN_model2, train_loader_long, optimizer3, criterion)\n","    valid_loss, valid_acc = evaluate(binary_FNN_model2, test_loader_long, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  from ipykernel import kernelapp as app\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 1m 24s\n","\tTrain Loss: 0.425 | Train Acc: 81.88%\n","\tTest Loss: 0.407 |  Test Acc: 82.62%\n","Epoch: 02 | Epoch Time: 1m 22s\n","\tTrain Loss: 0.402 | Train Acc: 83.29%\n","\tTest Loss: 0.382 |  Test Acc: 83.81%\n"]}]},{"cell_type":"markdown","metadata":{"id":"jV4e2ielKIOv"},"source":["### (a) Part 2: FF Network for Ternary Classiffication"]},{"cell_type":"code","metadata":{"id":"hG_snMPN5pK0"},"source":["##################################################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fgKyrftnwYx"},"source":["#def accuracy(predictions, labels):\n","#    classes = torch.argmax(predictions, dim=1)\n","#    return torch.mean((classes == labels).float())\n","def accuracy(predictions, labels):\n","    Y_prediction = labels\n","    accuracy = ((Y_prediction.data == predictions.data).float().mean())    \n","    return accuracy.item()\n","\n","def categorical_accuracy(predictions, labels):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    top_pred = predictions.argmax(1, keepdim = True)\n","    correct = top_pred.eq(labels.view_as(top_pred)).sum()\n","    acc = correct.float() / labels.shape[0]\n","    return acc\n","\n","def one_hot_embedding(labels, num_classes):\n","    y = torch.eye(num_classes) \n","    return y[labels] \n","\n","def train_model2(model, x, y, test_x, test_y, optimizer, epochs = 30):\n","    running_accuracy = 0.00\n","    for epoch in range(epochs):\n","        # calculate the loss from forward pass \n","        loss = criterion(model(x), y)\n","        loss_test = criterion(model(test_x), test_y)\n","        # accuarcy \n","        train_acc = accuracy(model(x), y)\n","        train_acc_test = accuracy(model(test_x), test_y)\n","\n","        print('Epoch: {} \\tTrain Loss: {:.4f} \\tTrain Acc: {:.4f} \\tTest Loss: {:.4f} \\tTest Acc: {:.4f}'\n","        .format(epoch+1, loss, train_acc, loss_test, train_acc_test))\n","\n","        optimizer.zero_grad()\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","    #running_accuracy += accuracy(model(x), y)\n","    #print(\"The overall testing accuracy:\", running_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxNWJ7Nh8HVJ","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1633376557589,"user_tz":420,"elapsed":663,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"4fafd033-117d-4334-b32b-7d459651219e"},"source":["training_y_250k=torch.from_numpy(training_y_250k)\n","training_y_250k=torch.tensor(training_y_250k, dtype=torch.long)\n","\n","testing_y_250k=torch.from_numpy(testing_y_250k)\n","testing_y_250k=torch.tensor(testing_y_250k, dtype=torch.long)\n","\n","train_dat_250k = list(zip(training_X_250k,training_y_250k))\n","test_dat_250k = list(zip(testing_X_250k,testing_y_250k))\n","\n","# dataloaders\n","batch_size = 200\n","\n","# make sure to shuffle data\n","train_loader_250k = DataLoader(train_dat_250k, shuffle=True, batch_size=batch_size)\n","test_loader_250k = DataLoader(test_dat_250k, shuffle=True, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-99ccd329b4e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_y_250k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_y_250k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_y_250k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_y_250k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtesting_y_250k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_y_250k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtesting_y_250k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_y_250k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"]}]},{"cell_type":"markdown","metadata":{"id":"Ah9bTjGh-Mp7"},"source":["#### Model 3: Using google pretrained model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFRLaWUS6u4C","executionInfo":{"status":"ok","timestamp":1633376561921,"user_tz":420,"elapsed":157,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"f0d35ac1-4553-40a6-ba84-ea0ae489d870"},"source":["class ternary_FNN1(nn.Module):\n","    def __init__(self):\n","        super(ternary_FNN1, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(300, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 3)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(get_mean_vector(pretrained_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return x \n","\n","ternary_FNN_model1 = ternary_FNN1()\n","ternary_FNN_model1.to(device)\n","print(ternary_FNN_model1)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(ternary_FNN_model1.parameters(), lr=0.01)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ternary_FNN1(\n","  (fc1): Linear(in_features=300, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBzhZiAm63q9","executionInfo":{"status":"ok","timestamp":1633376668099,"user_tz":420,"elapsed":80365,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"14d42162-91e2-426d-fd4f-16e78fbbfa58"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(ternary_FNN_model1, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(ternary_FNN_model1, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 40s\n","\tTrain Loss: 0.811 | Train Acc: 65.23%\n","\tTest Loss: 0.776 |  Test Acc: 66.30%\n","Epoch: 02 | Epoch Time: 0m 39s\n","\tTrain Loss: 0.785 | Train Acc: 66.31%\n","\tTest Loss: 0.763 |  Test Acc: 67.33%\n"]}]},{"cell_type":"markdown","metadata":{"id":"ryrn-HIc-Mzs"},"source":["#### Model 4: Using my word2vec model"]},{"cell_type":"code","metadata":{"id":"nuWnHl1lJfpT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633376668099,"user_tz":420,"elapsed":12,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"be543320-2a0e-48dc-e089-17bfc9c83dcd"},"source":["class ternary_FNN2(nn.Module):\n","    def __init__(self):\n","        super(ternary_FNN2, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(300, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 3)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(get_mean_vector(w2v_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return x \n","\n","ternary_FNN_model2 = ternary_FNN2()\n","ternary_FNN_model2.to(device)\n","print(ternary_FNN_model2)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(ternary_FNN_model2.parameters(), lr=0.01)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ternary_FNN2(\n","  (fc1): Linear(in_features=300, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"Kh3kf0DLKbMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633376768519,"user_tz":420,"elapsed":100426,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"c3f2ec26-e5dd-4231-9fea-2d96a9855f79"},"source":["N_EPOCHS = 1\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(ternary_FNN_model2, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(ternary_FNN_model2, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  from ipykernel import kernelapp as app\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 1m 40s\n","\tTrain Loss: 0.766 | Train Acc: 67.12%\n","\tTest Loss: 0.728 |  Test Acc: 68.44%\n"]}]},{"cell_type":"markdown","metadata":{"id":"w3rvys3CDJag"},"source":["### (b) Part 1: FF Network for First 10 Word2Vec and Binary Classification\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"xlLj611uPD3k"},"source":["def first10_vector(model, words):\n","    word = []\n","    count = 0\n","    for i in words.split():\n","        if i in model:\n","            word.append(model[i])\n","            count += 1\n","            if count == 10:\n","                break\n","    while count != 10:\n","        word.append(np.zeros(300))\n","        count += 1\n","    #return word\n","    return np.reshape(word,3000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7f2q4sUn_6H"},"source":["### work on my word2vec\n","# apply first10_vector to 200k data \n","#first10_my_x_train  = []\n","#first10_my_x_test = []\n","#for x in training_X:\n","#    first10_my_x_train.append(first10_vector(w2v_model, x))\n","#for x in testing_X:\n","#    first10_my_x_test.append(first10_vector(w2v_model, x))\n","\n","# apply first10_vector to 250k data\n","#first10_my_x_train_250k  = []\n","#first10_my_x_test_250k = []\n","#for x in training_X_250k:\n","#    first10_my_x_train_250k.append(first10_vector(w2v_model, x))\n","#for x in testing_X_250k:\n","#    first10_my_x_test_250k.append(first10_vector(w2v_model, x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POUQTujLtBEp"},"source":["### work on google pretrainded word2vec\n","# apply mean vector to 200k data\n","#first10_google_x_train  = []\n","#first10_google_x_test = []\n","#for x in training_X:\n","#    first10_google_x_train.append(first10_vector(pretrained_model, x))\n","#for x in testing_X:\n","#    first10_google_x_test.append(first10_vector(pretrained_model, x))\n","\n","# apply mean vector to 200k data\n","#first10_google_x_train_250k  = []\n","#first10_google_x_test_250k = []\n","#for x in training_X_250k:\n","#    first10_google_x_train_250k.append(first10_vector(pretrained_model, x))\n","#for x in testing_X_250k:\n","#    first10_google_x_test_250k.append(first10_vector(pretrained_model, x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sp_MC9VXsm-K"},"source":["#### Model 5: Using google pretrained model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K18roHXy_3G4","executionInfo":{"status":"ok","timestamp":1633376768523,"user_tz":420,"elapsed":34,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"939bf7d0-872c-4520-a64c-c68852bc477b"},"source":["class binary_FNN3(nn.Module):\n","    def __init__(self):\n","        super(binary_FNN3, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(3000, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 2)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(first10_vector(pretrained_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        #embedded = [batch size, emb dim]\n","        #print(embedded.shape)\n","        # add hidden layer, with relu activation function\n","        x = embedded.view(-1,3000)\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.fc3(x)\n","        #x = F.tanh(self.fc2(x))  # tanh activation\n","        #print(x.shape)\n","        return x \n","        \n","binary_FNN_model3 = binary_FNN3()\n","binary_FNN_model3.to(device)\n","print(binary_FNN_model3)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(binary_FNN_model3.parameters(), lr=0.01)\n","optimizer2 = torch.optim.Adam(binary_FNN_model3.parameters(), lr=0.02)\n","optimizer3 = torch.optim.Adam(binary_FNN_model3.parameters(), lr=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["binary_FNN3(\n","  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYYvBPafAAQW","executionInfo":{"status":"ok","timestamp":1633376872314,"user_tz":420,"elapsed":25246,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"145c19e3-467b-4aa4-991c-d4c51533cc2f"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(binary_FNN_model3, train_loader_long, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(binary_FNN_model3, test_loader_long, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.488 | Train Acc: 75.98%\n","\tTest Loss: 0.470 |  Test Acc: 77.36%\n","Epoch: 02 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.443 | Train Acc: 78.85%\n","\tTest Loss: 0.462 |  Test Acc: 77.80%\n"]}]},{"cell_type":"markdown","metadata":{"id":"-bHVHrtfsDyO"},"source":["#### Model 6: Using my word2vec model"]},{"cell_type":"code","metadata":{"id":"LbtSDCDZ-jbS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633376905683,"user_tz":420,"elapsed":5,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"7b870dda-1bf1-4da3-f6a0-9272deca7457"},"source":["class binary_FNN4(nn.Module):\n","    def __init__(self):\n","        super(binary_FNN4, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(3000, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 2)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(first10_vector(w2v_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        #embedded = [batch size, emb dim]\n","        #print(embedded.shape)\n","        # add hidden layer, with relu activation function\n","        x = embedded.view(-1,3000)\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.fc3(x)\n","        #x = F.tanh(self.fc2(x))  # tanh activation\n","        #print(x.shape)\n","        return x \n","        \n","binary_FNN_model4 = binary_FNN4()\n","binary_FNN_model4.to(device)\n","print(binary_FNN_model4)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(binary_FNN_model4.parameters(), lr=0.01)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["binary_FNN4(\n","  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpmFzBJsCMue","executionInfo":{"status":"ok","timestamp":1633376953510,"user_tz":420,"elapsed":44137,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"5d71ce0f-d4ce-4c9a-8f5f-451686e3f8b0"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(binary_FNN_model4, train_loader_long, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(binary_FNN_model4, test_loader_long, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 22s\n","\tTrain Loss: 0.477 | Train Acc: 76.93%\n","\tTest Loss: 0.453 |  Test Acc: 78.53%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.428 | Train Acc: 79.98%\n","\tTest Loss: 0.448 |  Test Acc: 79.01%\n"]}]},{"cell_type":"markdown","metadata":{"id":"cG2W63EcKmDa"},"source":["### (b) Part 2: FF Network for First 10 Word2Vec and Ternary Classification"]},{"cell_type":"markdown","metadata":{"id":"Lp6BYWHCu6-D"},"source":["#### Model 7: Using google pretrained model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"js_dNQg_Cznb","executionInfo":{"status":"ok","timestamp":1633376961302,"user_tz":420,"elapsed":172,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"4d98107f-59e0-4d94-9d36-cbe744534217"},"source":["class ternary_FNN3(nn.Module):\n","    def __init__(self):\n","        super(ternary_FNN3, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(3000, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 3)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(first10_vector(pretrained_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        x = embedded.view(-1,3000)\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return x \n","\n","ternary_FNN_model3 = ternary_FNN3()\n","ternary_FNN_model3.to(device)\n","print(ternary_FNN_model3)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(ternary_FNN_model3.parameters(), lr=0.01)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ternary_FNN3(\n","  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6AvNF9U9DGFm","executionInfo":{"status":"ok","timestamp":1633376995525,"user_tz":420,"elapsed":31438,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"3e942007-801b-4c5e-9c4b-1e02ffbdd61a"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(ternary_FNN_model3, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(ternary_FNN_model3, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTestLoss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.885 | Train Acc: 60.42%\n","\tTestLoss: 0.848 |  Test Acc: 62.36%\n","Epoch: 02 | Epoch Time: 0m 15s\n","\tTrain Loss: 0.843 | Train Acc: 62.84%\n","\tTestLoss: 0.839 |  Test Acc: 62.81%\n"]}]},{"cell_type":"markdown","metadata":{"id":"2tvALTFPuz_R"},"source":["#### Model 8: Using my word2vec model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNrQLd-QDsMF","executionInfo":{"status":"ok","timestamp":1633377019755,"user_tz":420,"elapsed":167,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"e8b9c7cd-54ee-440d-e4ef-68812148ae00"},"source":["class ternary_FNN4(nn.Module):\n","    def __init__(self):\n","        super(ternary_FNN4, self).__init__()\n","        # number of hidden nodes in each layer 50 and 10\n","        hidden_1 = 50\n","        hidden_2 = 10\n","\n","        self.fc1 = nn.Linear(3000, hidden_1)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        self.fc3 = nn.Linear(hidden_2, 3)\n","        # dropout layer (p=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, text):\n","        embedded = []\n","        for i in text:\n","            embedded.append(first10_vector(w2v_model, i))  \n","        embedded=torch.from_numpy(np.asarray(embedded)).float().to(device)\n","        x = embedded.view(-1,3000)\n","        x = F.relu(self.fc1(embedded))\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return x \n","\n","ternary_FNN_model4 = ternary_FNN4()\n","ternary_FNN_model4.to(device)\n","print(ternary_FNN_model4)\n","#loss function \n","criterion = nn.CrossEntropyLoss()\n","#optimizer Adam and learning rate \n","optimizer = torch.optim.Adam(ternary_FNN_model4.parameters(), lr=0.01)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ternary_FNN4(\n","  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n","  (fc2): Linear(in_features=50, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gqgtZmCD2X4","executionInfo":{"status":"ok","timestamp":1633377079599,"user_tz":420,"elapsed":55516,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"afbabc7c-0138-463a-943b-e51f1418b92d"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(ternary_FNN_model4, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(ternary_FNN_model4, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.889 | Train Acc: 60.53%\n","\t Val. Loss: 0.847 |  Val. Acc: 62.50%\n","Epoch: 02 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.847 | Train Acc: 62.78%\n","\t Val. Loss: 0.837 |  Val. Acc: 62.85%\n"]}]},{"cell_type":"markdown","metadata":{"id":"04BdMpjcB7Gb"},"source":["## 5. Recurrent Neural Networks"]},{"cell_type":"code","metadata":{"id":"5AlSefKfoXFd"},"source":["def trunc_padding_review(model, words):\n","    word = []\n","    count = 0\n","    for i in words.lower().split():\n","        if i in model:\n","            word.append(model[i])\n","        else:\n","            word.append(np.zeros(300))\n","        count += 1\n","        if count == 20:\n","            break\n","    while count != 20:\n","        # adding np.zeros in front of word vector\n","        word.insert(0, np.zeros(300))\n","        #word.append(np.zeros(300))\n","        count += 1\n","    return word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaY_MgEpovn2"},"source":["training_X = training_review[\"review_body\"]#.to_list()  #training_review\n","training_y = training_review[\"sentiment\"].values\n","\n","testing_X = testing_review[\"review_body\"]#.to_list()  #testing_review\n","testing_y = testing_review[\"sentiment\"].values\n","\n","train_dat = list(zip(training_X,training_y))\n","test_dat = list(zip(testing_X,testing_y))\n","\n","# dataloaders\n","batch_size = 200\n","\n","# make sure to shuffle data\n","train_loader = DataLoader(train_dat, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_dat, shuffle=True, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__RBP5nDCBcg"},"source":["### (a) Part 1: RNN for Binary Classification"]},{"cell_type":"code","metadata":{"id":"crcmusH1r8fW"},"source":["def train2(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch_id, (text, y) in enumerate(iterator):\n","        \n","        optimizer.zero_grad()\n","        predictions = model(text).squeeze(1)\n","        loss = criterion(predictions, y.to(device))\n","        acc = binary_accuracy(predictions, y.to(device))\n","        \n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        #print(f'Batch ID: {batch_id}/{len(iterator)}, Training Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate2(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch_id, (text, y) in enumerate(iterator):\n","\n","            predictions = model(text).squeeze(1)\n","            loss = criterion(predictions, y.to(device))\n","            acc = binary_accuracy(predictions, y.to(device))\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            #print(f'Batch ID: {batch_id}/{len(iterator)}, Test Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RTOdTXRzP72"},"source":["#### Model 1: Using google pretrained model"]},{"cell_type":"code","metadata":{"id":"jEvqvH9THcr7"},"source":["class RNN1(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        \n","        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","      \n","    def forward(self, text):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append(trunc_padding_review(pretrained_model, x))        \n","        #embedded = [sent len, batch size, emb dim]\n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)\n","        # here text is embedded since the input training data has been embedded \n","        output, hidden = self.rnn(embedded.to(device))\n","        #output = [sent len, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","\n","        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","        return self.fc(hidden.squeeze(0))\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 1\n","\n","model1 = RNN1(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model1.to(device)\n","\n","optimizer = optim.Adam(model1.parameters(), lr=3e-3)\n","criterion = nn.BCEWithLogitsLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XCTlQArHhq6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633377160110,"user_tz":420,"elapsed":35544,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"c49fcf0b-4888-4c32-8b58-9747ddcc8137"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train2(model1, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate2(model1, test_loader, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.491 | Train Acc: 77.07%\n","\tTest Loss: 0.475 |  Test Acc: 78.39%\n","Epoch: 02 | Epoch Time: 0m 17s\n","\tTrain Loss: 0.476 | Train Acc: 78.21%\n","\tTest Loss: 0.470 |  Test Acc: 78.36%\n"]}]},{"cell_type":"markdown","metadata":{"id":"ksnsdqtozZLt"},"source":["#### Model 2: Using my work2vec model"]},{"cell_type":"code","metadata":{"id":"637C6zA4GOt7"},"source":["class RNN2(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        \n","        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","      \n","    def forward(self, text):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append(trunc_padding_review(w2v_model, x))        \n","        #embedded = [sent len, batch size, emb dim]\n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)\n","        # here text is embedded since the input training data has been embedded \n","        output, hidden = self.rnn(embedded.to(device))\n","        #output = [sent len, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","\n","        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","        return self.fc(hidden.squeeze(0))\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 1\n","\n","model2 = RNN2(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model2.to(device)\n","\n","optimizer = optim.Adam(model2.parameters(), lr=3e-3)\n","criterion = nn.BCEWithLogitsLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FuE5vuaBGwrX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633377251601,"user_tz":420,"elapsed":66295,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"f68cc8ff-dce9-4c0e-c13a-629fc92744e4"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train2(model2, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate2(model2, test_loader, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\tTest Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 32s\n","\tTrain Loss: 0.532 | Train Acc: 74.58%\n","\tTest Loss: 0.498 |  Test Acc: 77.40%\n","Epoch: 02 | Epoch Time: 0m 33s\n","\tTrain Loss: 0.504 | Train Acc: 76.93%\n","\tTest Loss: 0.568 |  Test Acc: 71.45%\n"]}]},{"cell_type":"markdown","metadata":{"id":"Te2DYmWz0uJ2"},"source":["### (a) Part 2: RNN for Ternary Classification"]},{"cell_type":"markdown","metadata":{"id":"TS7CoUeo1dcW"},"source":["#### Model 3: Using google pretrained model"]},{"cell_type":"code","metadata":{"id":"XnPafnOpt7tX"},"source":["class RNN3(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        \n","        super().__init__()\n","        \n","        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","      \n","    def forward(self, text):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append(trunc_padding_review(pretrained_model, x))        \n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        output, hidden = self.rnn(embedded.to(device))\n","        #output = [sent len, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","\n","        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","        \n","        return self.fc(hidden.squeeze(0))\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 3\n","\n","model3 = RNN3(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model3.to(device)\n","\n","optimizer = optim.Adam(model3.parameters(), lr=3e-3)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5QNw7SU4VLP","executionInfo":{"status":"ok","timestamp":1633377556724,"user_tz":420,"elapsed":43895,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"78e7a0a9-94ff-4929-d7d5-81a0f1ea92d0"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model3, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model3, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.835 | Train Acc: 63.32%\n","\t Val. Loss: 0.818 |  Val. Acc: 64.28%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.829 | Train Acc: 63.64%\n","\t Val. Loss: 0.798 |  Val. Acc: 65.42%\n"]}]},{"cell_type":"markdown","metadata":{"id":"wZ75ww3o1Z3r"},"source":["#### Model 4: Using my work2vec model"]},{"cell_type":"code","metadata":{"id":"U1GPWSBQ4JJK"},"source":["class RNN4(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim):\n","        \n","        super().__init__()\n","        \n","        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","      \n","    def forward(self, text):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append(trunc_padding_review(w2v_model, x))        \n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)\n","        #embedded = [sent len, batch size, emb dim]\n","\n","        output, hidden = self.rnn(embedded.to(device))\n","        #output = [sent len, batch size, hid dim]\n","        #hidden = [1, batch size, hid dim]\n","\n","        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","        \n","        return self.fc(hidden.squeeze(0))\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 3\n","\n","model4 = RNN4(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model4.to(device)\n","\n","optimizer = optim.Adam(model4.parameters(), lr=3e-3)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTLo4z_KuJnp","executionInfo":{"status":"ok","timestamp":1633377647106,"user_tz":420,"elapsed":82239,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"4e7c8071-43d0-42c7-c8a7-372b5f848ee0"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model4, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model4, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 40s\n","\tTrain Loss: 0.923 | Train Acc: 59.07%\n","\t Val. Loss: 0.893 |  Val. Acc: 61.30%\n","Epoch: 02 | Epoch Time: 0m 41s\n","\tTrain Loss: 0.897 | Train Acc: 60.96%\n","\t Val. Loss: 0.886 |  Val. Acc: 61.89%\n"]}]},{"cell_type":"markdown","metadata":{"id":"GFKA846zCICX"},"source":["### (b) Part 1: Gated RNN Binary Classification"]},{"cell_type":"code","metadata":{"id":"zPAjHDts6Xwp"},"source":["def train3(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch_id, (text, y) in enumerate(iterator):\n","        \n","        h = model.init_hidden(batch_size)\n","        h = h.data\n","        \n","        optimizer.zero_grad()\n","        predictions = model(text,h).squeeze(1)\n","        # print(predictions.shape)\n","        loss = criterion(predictions, y.to(device)) \n","        acc = binary_accuracy(predictions, y.to(device)) \n","        \n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        #print(f'Batch ID: {batch_id}/{len(iterator)}, Training Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate3(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch_id, (text, y) in enumerate(iterator):\n","            h = model.init_hidden(batch_size)\n","            h = h.data\n","\n","            predictions = model(text,h).squeeze(1)\n","            loss = criterion(predictions, y.to(device)) \n","            acc = binary_accuracy(predictions, y.to(device)) \n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            #print(f'Batch ID: {batch_id}/{len(iterator)}, Test Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZR6ADmv1zEL"},"source":["#### Model 5: Using google pretrained model"]},{"cell_type":"code","metadata":{"id":"Jm7S49Jp5aYT"},"source":["class GRUNet1(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers = 2, dropout=0.1):\n","        \n","        super().__init__()\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","\n","        self.GRN = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, text, h):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append( trunc_padding_review(pretrained_model, x) )  \n","\n","        # embedded = [sent len, batch size, hidden dim]\n","        # h = [layer, batch size, hidden dim]\n","        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)      \n","\n","        #print(embedded.shape)\n","        #print(h.shape)\n","        output, hidden = self.GRN(embedded.to(device), h)\n","        # output = [sent len, batch size, hidden dim] \n","        return self.fc(self.relu(output[-1,:]))\n","\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","\n","        return hidden\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 1\n","\n","model5 = GRUNet1(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model5.to(device)\n","optimizer = optim.Adam(model5.parameters(), lr=3e-3)\n","criterion = nn.BCEWithLogitsLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GePPcdg35oZ6","executionInfo":{"status":"ok","timestamp":1633378122466,"user_tz":420,"elapsed":37869,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"e5537ff7-413b-4845-bca1-e8e4a32ae111"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train3(model5, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate3(model5, test_loader, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.402 | Train Acc: 81.62%\n","\t Val. Loss: 0.363 |  Val. Acc: 83.81%\n","Epoch: 02 | Epoch Time: 0m 18s\n","\tTrain Loss: 0.343 | Train Acc: 84.91%\n","\t Val. Loss: 0.338 |  Val. Acc: 85.16%\n"]}]},{"cell_type":"markdown","metadata":{"id":"HRTY_-qs12XN"},"source":["#### Model 6: Using my work2vec model"]},{"cell_type":"code","metadata":{"id":"gY-LY8ly5Ou4"},"source":["class GRUNet2(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers = 2, dropout=0.1):\n","        \n","        super().__init__()\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","\n","        self.GRN = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, text, h):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append( trunc_padding_review(w2v_model, x) )  \n","\n","        # embedded = [sent len, batch size, hidden dim]\n","        # h = [layer, batch size, hidden dim]\n","        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)      \n","\n","        #print(embedded.shape)\n","        #print(h.shape)\n","        output, hidden = self.GRN(embedded.to(device), h)\n","        # output = [sent len, batch size, hidden dim] \n","        return self.fc(self.relu(output[-1,:]))\n","\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","\n","        return hidden\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 1\n","\n","model6 = GRUNet2(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model6.to(device)\n","optimizer = optim.Adam(model6.parameters(), lr=3e-3)\n","criterion = nn.BCEWithLogitsLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQ8lr1yu6_61","executionInfo":{"status":"ok","timestamp":1633378506269,"user_tz":420,"elapsed":66878,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"dffb45eb-d485-483c-d9c7-405063513867"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train3(model6, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate3(model6, test_loader, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 33s\n","\tTrain Loss: 0.374 | Train Acc: 83.20%\n","\t Val. Loss: 0.344 |  Val. Acc: 84.90%\n","Epoch: 02 | Epoch Time: 0m 33s\n","\tTrain Loss: 0.331 | Train Acc: 85.52%\n","\t Val. Loss: 0.342 |  Val. Acc: 85.10%\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZFEUK46g181-"},"source":["### (b) Part 2: Gated RNN for Ternary Classification"]},{"cell_type":"code","metadata":{"id":"3yHmXxhb9WSH"},"source":["def train4(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch_id, (text, y) in enumerate(iterator):\n","        \n","        h = model.init_hidden(batch_size)\n","        h = h.data\n","        \n","        optimizer.zero_grad()\n","        predictions = model(text,h).squeeze(1)\n","        # print(predictions.shape)\n","        loss = criterion(predictions, y.to(device)) \n","        acc = categorical_accuracy(predictions, y.to(device)) \n","        \n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        #print(f'Batch ID: {batch_id}/{len(iterator)}, Training Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate4(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch_id, (text, y) in enumerate(iterator):\n","            h = model.init_hidden(batch_size)\n","            h = h.data\n","\n","            predictions = model(text,h).squeeze(1)\n","            loss = criterion(predictions, y.to(device)) \n","            acc = categorical_accuracy(predictions, y.to(device)) \n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            #print(f'Batch ID: {batch_id}/{len(iterator)}, Test Accuracy: {acc}')\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dODMkVOg2HpT"},"source":["#### Model 7: Using google pretrained model"]},{"cell_type":"code","metadata":{"id":"3Ww5UfLZ7KFe"},"source":["class GRUNet3(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers = 2, dropout=0.1):\n","        \n","        super().__init__()\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","\n","        self.GRN = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, text, h):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append(trunc_padding_review(pretrained_model, x))  \n","\n","        # embedded = [sent len, batch size, hidden dim]\n","        # h = [layer, batch size, hidden dim]\n","        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)      \n","\n","        #print(embedded.shape)\n","        #print(h.shape)\n","        output, hidden = self.GRN(embedded.to(device), h)\n","        # output = [sent len, batch size, hidden dim] \n","        return self.fc(self.relu(output[-1,:]))\n","\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","\n","        return hidden\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 3\n","\n","model7 = GRUNet3(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model7.to(device)\n","optimizer = optim.Adam(model7.parameters(), lr=3e-3)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2ul_Bni7Y6C","executionInfo":{"status":"ok","timestamp":1633378852061,"user_tz":420,"elapsed":47192,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"5474c6a7-24ff-4579-f7b9-45ad33ad2ab9"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train4(model7, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate4(model7, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.774 | Train Acc: 66.32%\n","\t Val. Loss: 0.726 |  Val. Acc: 68.62%\n","Epoch: 02 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.712 | Train Acc: 69.59%\n","\t Val. Loss: 0.709 |  Val. Acc: 69.71%\n"]}]},{"cell_type":"markdown","metadata":{"id":"Uh0GVvKD2LAW"},"source":["#### Model 8: Using my work2vec model"]},{"cell_type":"code","metadata":{"id":"7t-0PN-v94B3"},"source":["class GRUNet4(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers = 2, dropout=0.1):\n","        \n","        super().__init__()\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","\n","        self.GRN = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, text, h):\n","        #text = [sent len, batch size]\n","        embedded = []\n","        for x in text:\n","            embedded.append(trunc_padding_review(w2v_model, x))  \n","\n","        # embedded = [sent len, batch size, hidden dim]\n","        # h = [layer, batch size, hidden dim]\n","        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n","        embedded=torch.from_numpy(np.asarray(embedded)).float()\n","        embedded=embedded.permute(1, 0, 2)      \n","\n","        #print(embedded.shape)\n","        #print(h.shape)\n","        output, hidden = self.GRN(embedded.to(device), h)\n","        # output = [sent len, batch size, hidden dim] \n","        return self.fc(self.relu(output[-1,:]))\n","\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","\n","        return hidden\n","\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 3\n","\n","model8 = GRUNet4(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n","model8.to(device)\n","optimizer = optim.Adam(model8.parameters(), lr=3e-3)\n","criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPOAUCpM-ZZu","executionInfo":{"status":"ok","timestamp":1633379092551,"user_tz":420,"elapsed":84814,"user":{"displayName":"Jianing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwBriCZktMV2ErgkPrx8kF2di6IXmJ0-av1fULrA=s64","userId":"00484797517878318070"}},"outputId":"3ead42ba-8b18-4827-9ad7-049ebaa4aea9"},"source":["N_EPOCHS = 2\n","\n","best_valid_loss = float('inf')\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train4(model8, train_loader_250k, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate4(model8, test_loader_250k, criterion)\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 42s\n","\tTrain Loss: 0.747 | Train Acc: 67.86%\n","\t Val. Loss: 0.714 |  Val. Acc: 69.39%\n","Epoch: 02 | Epoch Time: 0m 42s\n","\tTrain Loss: 0.702 | Train Acc: 69.96%\n","\t Val. Loss: 0.703 |  Val. Acc: 69.86%\n"]}]}]}