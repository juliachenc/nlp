{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsL3bW6DDzhs",
    "outputId": "97235a0c-f25c-4ca2-d6fd-e2bb9b9c917e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/juliachen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliachen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6QOA0VbDzht",
    "outputId": "f7f060c4-2bba-4471-9a2a-b4df36477263"
   },
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "#! pip install contractions\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccuqKRrEDzht"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DA63ysmDDzhu",
    "outputId": "18b5d671-a614-4e3f-b6d3-51de7000b940"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amazon_reviews_us_Kitchen_v1_00.tsv\", \n",
    "                 sep='\\t',\n",
    "                 #usecols = ['star_rating','review_body'],\n",
    "                 error_bad_lines=False,\n",
    "                 warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMtm5zCZDzhu"
   },
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7AByehDeDzhu"
   },
   "outputs": [],
   "source": [
    "review_df = df[[\"star_rating\",\"review_body\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg0RNwhUDzhu"
   },
   "source": [
    "It important for us to check if there is any missing value in our dataset, beacause the missing value does not helpful to tell us any thing to predict the sentiment later.\n",
    "\n",
    "By calling `review_df.isnull().sum()`, there are 3 missing values on star_rating, and 246 missing values on review_body, so we will go head drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C0cAAiBNDzhv"
   },
   "outputs": [],
   "source": [
    "# drop na\n",
    "# review_df.isnull().sum()\n",
    "review_df_withoutmissing = review_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I personally have 5 days sets and have also bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fabulous and worth every penny. Used for clean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body\n",
       "0          5.0                Beautiful.  Looks great on counter.\n",
       "1          5.0  I personally have 5 days sets and have also bo...\n",
       "2          5.0  Fabulous and worth every penny. Used for clean..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# three sample reviews\n",
    "review_df_withoutmissing.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYA0HSkgyKxh"
   },
   "source": [
    "Now we can look at how many review in each rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icRqZvH5yKPG",
    "outputId": "70b2b3c6-a0f2-4603-8d1b-a55ebc2efa5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of review in each star rating: \n",
      " 5.0    3124595\n",
      "4.0     731701\n",
      "1.0     426870\n",
      "3.0     349539\n",
      "2.0     241939\n",
      "Name: star_rating, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of review in each star rating:\", \"\\n\" , \n",
    "      review_df_withoutmissing[\"star_rating\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1UjJM91ylae"
   },
   "source": [
    "There are 426870 reviews in 1 star, 241939 reviews in 2 stars, 349539 reviews in 3 stars, 731701 reviews in 4 stars, 3124595 reviews in 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoAqZ1cuDzhv"
   },
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ro2cpg0kDzhv"
   },
   "outputs": [],
   "source": [
    "# creat a new column named label \n",
    "review_df_new = review_df_withoutmissing.copy()\n",
    "review_df_new.loc[3, 'sentiment'] = None\n",
    "\n",
    "# get the index of row where star_rating is 4, 5\n",
    "# and get the index of row where star_rating is 1, 2\n",
    "row_idx_1 = review_df_new[review_df_new['star_rating']>=4].index\n",
    "row_idx_0 = review_df_new[review_df_new['star_rating']<=2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "n9zUo1V3Dzhw"
   },
   "outputs": [],
   "source": [
    "# assigning the labels \n",
    "review_df_new.loc[row_idx_1,'sentiment'] = 1\n",
    "review_df_new.loc[row_idx_0,'sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rd2mz4VCDzhw",
    "outputId": "3b001b0d-87c5-4388-e00a-ae2f29d9527d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3856296 positive reviews.\n"
     ]
    }
   ],
   "source": [
    "positive = review_df_new[review_df_new['sentiment']==1]\n",
    "print(\"There are\", positive['star_rating'].count(), \"positive reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxgGpOxcDzhw",
    "outputId": "c98f246c-9540-4187-d803-e19396c76fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 668809 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "negative = review_df_new[review_df_new['sentiment']==0]\n",
    "print(\"There are\", negative['star_rating'].count(), \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYn7zxcbDzhw",
    "outputId": "70737d7d-c163-479c-eead-81eb98f452bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 349539 reviews with the rating 3 stars. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "review3 = review_df_new[review_df_new['star_rating']==3]['star_rating']\n",
    "print(\"There are\", review3.count(), \"reviews with the rating 3 stars.\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_TZiNSa0adv"
   },
   "source": [
    "After we labeled reviews, we have 3856296 positive reviews, 668809 negative reviews, and 349539 reviews that got 3 stars which we will discard in later analysis. We can clearly see that our data now is imbalanced, since we have over 3 millon postive reviews, but only have less than 1 millon negative reviews.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brS_QANCDzhx"
   },
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ID2uEI1LDzhx"
   },
   "outputs": [],
   "source": [
    "# get 100,000 sample from both positive and negative reviews\n",
    "\n",
    "positive_sample = positive.sample(n = 100000, random_state=36)\n",
    "negative_sample = negative.sample(n = 100000, random_state=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0NS7YyFADzhx"
   },
   "outputs": [],
   "source": [
    "# combine two dataframes pandas and shuffle\n",
    "\n",
    "samples = pd.concat([positive_sample, negative_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "veVcSyTTDzhy"
   },
   "outputs": [],
   "source": [
    "# split data into 80% training dataset and 20% testing dataset.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_review, testing_review = train_test_split(samples, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 160000 training data, and 40000 testing data. We will build models using features from training and see how it works on the testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t7Z82PwDzhy"
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32vsW5Xe4EyW",
    "outputId": "1d3823e2-3570-46c2-e0d1-aa2603bdcd88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Review Before Data Cleaning:  323.05735\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Length of Review Before Data Cleaning: \",\n",
    "      (training_review[\"review_body\"].apply(len).sum() + \\\n",
    "       testing_review[\"review_body\"].apply(len).sum())/200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lSYeAUL8Dzhy"
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None \n",
    "training_review.loc[:, \"review_body\"] = training_review.loc[:, \"review_body\"].str.lower()\n",
    "testing_review.loc[:, \"review_body\"] = testing_review.loc[:, \"review_body\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqTnkBb0Dzhy"
   },
   "source": [
    "## remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LfflTo64GZrx"
   },
   "outputs": [],
   "source": [
    "# code ref: https://www.kaggle.com/hemrajsukriya/amazon-reviews-for-sentiment-analysis \n",
    "def remove_url(text):\n",
    "    url=re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\" \",text)\n",
    "def remove_html(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    return cleanr.sub(r\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9NKUGuO9QSOT"
   },
   "outputs": [],
   "source": [
    "# review url and html and url\n",
    "testing_review[\"review_body\"] = testing_review[\"review_body\"].map(lambda x:remove_url(x))\n",
    "testing_review[\"review_body\"] = testing_review[\"review_body\"].map(lambda x:remove_html(x))\n",
    "training_review[\"review_body\"] = training_review[\"review_body\"].map(lambda x:remove_url(x))\n",
    "training_review[\"review_body\"] = training_review[\"review_body\"].map(lambda x:remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcxAV76hDzh0"
   },
   "source": [
    "## perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vHFRHgbXDzh0"
   },
   "outputs": [],
   "source": [
    "#code ref: https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/\n",
    "import contractions\n",
    "\n",
    "training_review[\"review_body\"] = training_review[\"review_body\"].apply(\n",
    "    lambda x: ' '.join(contractions.fix(word) for word in x.split()))\n",
    "testing_review[\"review_body\"] = testing_review[\"review_body\"].apply(\n",
    "    lambda x: ' '.join(contractions.fix(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlBVlfuPDzhz"
   },
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zhTLC9kwDzhz"
   },
   "outputs": [],
   "source": [
    "regex = '[^a-zA-Z]'\n",
    "training_review[\"review_body\"] = training_review[\"review_body\"].replace(regex, ' ', regex=True)\n",
    "testing_review[\"review_body\"] = testing_review[\"review_body\"].replace(regex, ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOu09gBkDzhz"
   },
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dKrYw8sHDzh0"
   },
   "outputs": [],
   "source": [
    "#code reference: https://stackoverflow.com/questions/43071415/remove-multiple-blanks-in-dataframe\n",
    "\n",
    "training_review[\"review_body\"] = training_review[\"review_body\"].replace('\\s+', ' ', regex=True)\n",
    "testing_review[\"review_body\"] = testing_review[\"review_body\"].replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdjH9eON3pD5",
    "outputId": "adf2bf95-0300-455d-bc64-ab70e1637109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Review After Data Cleaning:  309.89008\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Length of Review After Data Cleaning: \",\n",
    "      (training_review[\"review_body\"].apply(len).sum() + \\\n",
    "       testing_review[\"review_body\"].apply(len).sum())/200000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXvJ67PRDzh1"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EI91hhbj4jo-",
    "outputId": "5c1108fe-c9ff-4218-89e6-0688519223cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Review Before Pre-processing:  309.89008\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Length of Review Before Pre-processing: \",\n",
    "      (training_review[\"review_body\"].apply(len).sum()+\\\n",
    "       testing_review[\"review_body\"].apply(len).sum())/200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIQwwqYxDzh1"
   },
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0NCZNK8rDzh1"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "training_review[\"review_body\"] = training_review[\"review_body\"].apply(\n",
    "    lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "testing_review[\"review_body\"] = testing_review[\"review_body\"].apply(\n",
    "    lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idw2sOtJDzh2"
   },
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DnO_6Hj8Dzh2"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "training_review[\"review_body\"] = training_review[\"review_body\"].apply(\n",
    "    lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "testing_review[\"review_body\"] = testing_review[\"review_body\"].apply(\n",
    "    lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcEcSVKM3ce1",
    "outputId": "15d82ab8-e357-4e9e-9c66-e7d310bf0976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length of Review After Pre-processing:  189.89327 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Length of Review After Pre-processing: \",\n",
    "      (training_review[\"review_body\"].apply(len).sum()+\\\n",
    "       testing_review[\"review_body\"].apply(len).sum())/200000, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KkoABtuDzh2"
   },
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GoU5ydgbDzh2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# convert raw review_body to a matrix of TF-IDF features\n",
    "tfidf = TfidfVectorizer(ngram_range = (1, 2))\n",
    "\n",
    "\n",
    "# transform training_review and testing_review to vectorized train_x_tfidf and test_x_tfidf\n",
    "train_x_tfidf = tfidf.fit_transform(np.array(training_review[\"review_body\"]))\n",
    "test_x_tfidf = tfidf.transform(np.array(testing_review[\"review_body\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hn-B17idgbP"
   },
   "source": [
    "The default **TfidfVectorizer** have incorporated smooth_idf and use_idf. I used (1,2) ngram range in my case, which means both unigram and bigram will be used. N-gram refers to a string of n words in a row, so now, 1 word and 2 words will be considered. I use this is because it can boosted my prediction. It will make sense, since there are many words will always appear together.  \n",
    "\n",
    "\n",
    "**tfidf.fit** function will calculate the parameters from the data, the **tfidf.transform** is basically applying the parameters to the data, **tfidf.fit_ransform** combine fit and transform function. In our case, we will have to get all necessary information from train data then apply it to test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrXqj6IvDzh4"
   },
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVpouj_HDzh4",
    "outputId": "db584780-bb18-4004-cb6b-1e9534e3d439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# fit the training data on the classifier\n",
    "Percet= Perceptron()\n",
    "Percet.fit(train_x_tfidf,training_review[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron is a single neuron that can be used for classification problems. We may consider it as a linear model, but it is bit different from linear regression. the perceptron predicts a binary class label with $\\pm(w^Tx_i)$ , whereas linear regression predicts a real value with $w^Tx_i$. In my perceptron model, I used defalut parameters, where is no penalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2cfETzIaDzh4"
   },
   "outputs": [],
   "source": [
    "# predict the labels on traning data\n",
    "predictions_Percet_train = Percet.predict(train_x_tfidf)\n",
    "# predict the labels on testing data\n",
    "predictions_Percet_test = Percet.predict(test_x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0n8H8dfDzh4",
    "outputId": "685bd6ad-d6d0-4692-ab71-efa077d553e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Perceptron Accuracy Score: 0.99358125\n",
      "Training Perceptron Precision Score: 0.9937630457334983\n",
      "Training Perceptron Recall Score: 0.9934029686641012\n",
      "Training Perceptron F1 Score: 0.9935829745755829 \n",
      "\n",
      "Testing Perceptron Accuracy Score: 0.89785\n",
      "Testing Perceptron Precision Score: 0.9022657930275596\n",
      "Testing Perceptron Recall Score: 0.8943037033364731\n",
      "Testing Perceptron F1 Score: 0.8982671048700328 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Training Perceptron Accuracy Score:\", accuracy_score(predictions_Percet_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Perceptron Precision Score:\",precision_score(predictions_Percet_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Perceptron Recall Score:\",recall_score(predictions_Percet_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Perceptron F1 Score:\", f1_score(predictions_Percet_train, training_review[\"sentiment\"]),\"\\n\")\n",
    "\n",
    "print(\"Testing Perceptron Accuracy Score:\", accuracy_score(predictions_Percet_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Perceptron Precision Score:\",precision_score(predictions_Percet_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Perceptron Recall Score:\",recall_score(predictions_Percet_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Perceptron F1 Score:\", f1_score(predictions_Percet_test, testing_review[\"sentiment\"]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list of prediction scores, we can see that all training prediction scores are all over 99%. Testing prediction scores are a bit lower than training which tells us that our model is not overfitting and 90% also looks very decent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAdNUYjrDzh5"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0H1n0ZjDzh5",
    "outputId": "75624124-54c0-495c-8b56-5fa5ea9d4a26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# fit the training data on the classifier\n",
    "SVM = svm.LinearSVC()\n",
    "SVM.fit(train_x_tfidf,training_review[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq3hbQt46WQ7"
   },
   "source": [
    "LinearSVC tends to be faster to converge given a large sample because it can deal with sparse dataset, in our case the traning data is very large, when fitting to the regular svc model with linear kernel will be hard to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Y5C7hhfHDzh5"
   },
   "outputs": [],
   "source": [
    "# predict the labels on traning data\n",
    "predictions_SVM_train = SVM.predict(train_x_tfidf)\n",
    "# predict the labels on testing data\n",
    "predictions_SVM_test = SVM.predict(test_x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuwYp-f2Dzh5",
    "outputId": "f03008f9-1f42-4895-cc9d-480fc5acc519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM Accuracy Score: 0.9949125\n",
      "Training SVM Precision Score: 0.9961253390328346\n",
      "Training SVM Recall Score: 0.9937157890799366\n",
      "Training SVM F1 Score: 0.9949191051632877 \n",
      "\n",
      "Testing SVM Accuracy Score: 0.91585\n",
      "Testing SVM Precision Score: 0.9165207822737959\n",
      "Testing SVM Recall Score: 0.9152389990509965\n",
      "Testing SVM F1 Score: 0.9158794421952317 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Training SVM Accuracy Score:\", accuracy_score(predictions_SVM_train, training_review[\"sentiment\"]))\n",
    "print(\"Training SVM Precision Score:\",precision_score(predictions_SVM_train, training_review[\"sentiment\"]))\n",
    "print(\"Training SVM Recall Score:\",recall_score(predictions_SVM_train, training_review[\"sentiment\"]))\n",
    "print(\"Training SVM F1 Score:\", f1_score(predictions_SVM_train, training_review[\"sentiment\"]), \"\\n\")\n",
    "\n",
    "print(\"Testing SVM Accuracy Score:\", accuracy_score(predictions_SVM_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing SVM Precision Score:\",precision_score(predictions_SVM_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing SVM Recall Score:\",recall_score(predictions_SVM_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing SVM F1 Score:\", f1_score(predictions_SVM_test, testing_review[\"sentiment\"]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, all training prediction scores are all over 99%; all testing prediction scores are a bit lower than training data. Precision and recall are very close, which means the number of false negative and false positive are similar. The overall accuracy is over 91.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU9bk-ViDzh5"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfa1rMqxDzh5",
    "outputId": "acfe750a-a9c5-4036-d590-a70cb37a2e0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, solver='liblinear')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit the training data on the classifier\n",
    "Logit= LogisticRegression(C = 10, penalty = 'l2', solver = 'liblinear')\n",
    "Logit.fit(train_x_tfidf,training_review[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear model, I applied GridSearch to select hyper-parameters. C is the inverse regularization parameter, and the larger C the less penalty for the parameters norm function. The penatly I choose L2 norm, which add squared magnitude as penalty to the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "SOBFBQ-3Dzh6"
   },
   "outputs": [],
   "source": [
    "# predict the labels on traning data\n",
    "predictions_Logit_train = Logit.predict(train_x_tfidf)\n",
    "# predict the labels on testing data\n",
    "predictions_Logit_test = Logit.predict(test_x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlV8B06DDzh6",
    "outputId": "61906de2-6713-466e-d9de-fe3c6fc0bdfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Accuracy Score: 0.99331875\n",
      "Training Logistic Precision Score: 0.9944504855825115\n",
      "Training Logistic Recall Score: 0.9922058163316206\n",
      "Training Logistic F1 Score: 0.9933268828615125 \n",
      "\n",
      "Testing Logistic Accuracy Score: 0.916625\n",
      "Testing Logistic Precision Score: 0.9161706597309058\n",
      "Testing Logistic Recall Score: 0.9169503404084902\n",
      "Testing Logistic F1 Score: 0.9165603342590508 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model accuracy\n",
    "print(\"Training Logistic Accuracy Score:\", accuracy_score(predictions_Logit_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Logistic Precision Score:\",precision_score(predictions_Logit_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Logistic Recall Score:\",recall_score(predictions_Logit_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Logistic F1 Score:\", f1_score(predictions_Logit_train, training_review[\"sentiment\"]), \"\\n\")\n",
    "\n",
    "print(\"Testing Logistic Accuracy Score:\", accuracy_score(predictions_Logit_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Logistic Precision Score:\",precision_score(predictions_Logit_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Logistic Recall Score:\",recall_score(predictions_Logit_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Logistic F1 Score:\", f1_score(predictions_Logit_test, testing_review[\"sentiment\"]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of logistic regression are quiet simiar to LinearSVC. Again, all training prediction scores are all over 99%. Testing accuracy, precision, recall and F1 scores are all over 91.6%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qriqsi_Dzh7"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJ3tdsdjDzh7",
    "outputId": "0e86bdc5-3c8e-4c85-f768-3fb769e290f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# fit the training data on the classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(train_x_tfidf,training_review[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier's idea is created based on Bayesian Theorem1. The fundamental assumption hold by this classifier is to treat each word independently. That is to say, the Naive Bayes classifier will ignore all rules, such as the occurrence of one word does not  affect the probability of the other word's happening but keeping track of the likelihood of\n",
    "the labels by given words or phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "xmbUd_mYDzh7"
   },
   "outputs": [],
   "source": [
    "# predict the labels on training data\n",
    "predictions_Naive_train = Naive.predict(train_x_tfidf)\n",
    "# predict the labels on testing data\n",
    "predictions_Naive_test = Naive.predict(test_x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFNsojNqDzh7",
    "outputId": "1d228771-0a56-4454-a5e8-0e21128fb15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes Accuracy Score: 0.95515\n",
      "Training Naive Bayes Precision Score: 0.9438299148824478\n",
      "Training Naive Bayes Recall Score: 0.9657011317859198\n",
      "Training Naive Bayes F1 Score: 0.9546402700345128 \n",
      "\n",
      "Testing Naive Bayes Accuracy Score: 0.8967\n",
      "Testing Naive Bayes Precision Score: 0.8679037663182114\n",
      "Testing Naive Bayes Recall Score: 0.9208724725362204\n",
      "Testing Naive Bayes F1 Score: 0.8936038726954373 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model accuracy\n",
    "print(\"Training Naive Bayes Accuracy Score:\", accuracy_score(predictions_Naive_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Naive Bayes Precision Score:\",precision_score(predictions_Naive_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Naive Bayes Recall Score:\",recall_score(predictions_Naive_train, training_review[\"sentiment\"]))\n",
    "print(\"Training Naive Bayes F1 Score:\", f1_score(predictions_Naive_train, training_review[\"sentiment\"]), \"\\n\")\n",
    "\n",
    "print(\"Testing Naive Bayes Accuracy Score:\", accuracy_score(predictions_Naive_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Naive Bayes Precision Score:\",precision_score(predictions_Naive_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Naive Bayes Recall Score:\",recall_score(predictions_Naive_test, testing_review[\"sentiment\"]))\n",
    "print(\"Testing Naive Bayes F1 Score:\", f1_score(predictions_Naive_test, testing_review[\"sentiment\"]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to all three models above, the Naive Bayes model is a bit worse. The testing accuracy is close to the F1 score, but the precision score is lower than recall. It tells us that false positive is higher than false negative. The testing prediction result shows that the precision score is 86.79%, but the recall rate is 92.87%. Therefore, we can conclude that the model yields more false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, logistic regression gives the highest scores in all testing prediction scores (91.6%). LinearSVC also gives good results on all prediction scores (91.5%). Perceptron is also reasonable, which gives about 90% on all scores. For those three models, the training accuracy are all reach 99%. The naive Bayes model has imbalanced precision and recall on training data, and this is also amplified on the test set. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8t7Z82PwDzhy",
    "lXvJ67PRDzh1"
   ],
   "name": "HW1-CSCI544.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
