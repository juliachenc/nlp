{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: NER via Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.style.use('seaborn-pastel')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"./data/train\" \n",
    "parameters['dev'] = \"./data/dev\" \n",
    "parameters['test'] = \"./data/test\" \n",
    "parameters['lower'] = True \n",
    "parameters['zeros'] =  False \n",
    "parameters['word_dim'] = 100 \n",
    "parameters['word_lstm_dim'] = 256 \n",
    "parameters['word_bidirect'] = True\n",
    "parameters['embedding_path'] = \"./glove.6B.100d.txt\" \n",
    "parameters['dropout'] = 0.33 \n",
    "parameters['epoch'] =  20\n",
    "parameters['gradient_clip']=5.0\n",
    "models_path = \"./models/\" \n",
    "\n",
    "#GPU\n",
    "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
    "use_gpu = parameters['use_gpu']\n",
    "\n",
    "#parameters['reload'] = \"./models/pre-trained-model\" \n",
    "parameters['reload'] = False\n",
    "#Constants\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
    "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
    "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Mappings for Words and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[1].lower() if lower else x[1] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21011 unique words (204567 in total)\n",
      "Found 11 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function prepare dataset returns a list of dictionaries ( one dictionary per each sentence )\n",
    "\n",
    "Each of the dictionary returned by the function contains\n",
    "  1. list of all words in the sentence\n",
    "  2. list of word index for all words in the sentence\n",
    "  3. list of lists, containing character id of each character for words in the sentence\n",
    "  4. list of tag for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(x, lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14987 / 3466 / 3684 sentences in train / dev / train.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(sentences, word_to_id, tag_to_id, lower=False, test=0):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[1] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        if test == 0:\n",
    "            tags = [tag_to_id[w[-1]] for w in s]\n",
    "            data.append({\n",
    "                'str_words': str_words,\n",
    "                'words': words,\n",
    "                #'chars': chars,\n",
    "                'tags': tags,\n",
    "            })\n",
    "        else:\n",
    "            data.append({\n",
    "                'str_words': str_words,\n",
    "                'words': words,\n",
    "            })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, tag_to_id, parameters['lower'], test = 1\n",
    ")\n",
    "print(\"{} / {} / {} sentences in train / dev / train.\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Embeddings\n",
    "\n",
    "Now, We will randomly embed our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15803386,  0.04817372, -0.08887219, ..., -0.23762649,\n",
       "         0.20478912, -0.2153611 ],\n",
       "       [-0.1243387 ,  0.07428248,  0.14206932, ..., -0.12125302,\n",
       "        -0.20020828,  0.02645931],\n",
       "       [-0.01706656, -0.0725474 ,  0.10472512, ...,  0.09301148,\n",
       "        -0.05962866, -0.07434036],\n",
       "       ...,\n",
       "       [-0.02680391,  0.15265586, -0.15616517, ...,  0.1653538 ,\n",
       "         0.00999025,  0.15492515],\n",
       "       [-0.03792276,  0.18925285, -0.02060655, ...,  0.0600599 ,\n",
       "         0.05767155, -0.13615273],\n",
       "       [ 0.05666281,  0.03390512,  0.18806308, ...,  0.24323237,\n",
       "         0.09536502,  0.24343882]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
    "                  pre_word_embeds=None, use_gpu=False):\n",
    "\n",
    "        \n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        #parameter initialization for the model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        \n",
    "        ### Layer 1: Embedding :\n",
    "        #Word Embedding\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            #Initializes the word embeddings with pretrained word embeddings\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "        \n",
    "    \n",
    "        #Initializing the dropout layer, with dropout specificed in parameters\n",
    "        self.dropout = nn.Dropout(parameters['dropout'])\n",
    "        \n",
    "        ### Layer 2: Lstm :\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1,\n",
    "                            bidirectional=True,\n",
    "                            )    \n",
    "\n",
    "        ### Layer 3: Linear + ELU\n",
    "        #Linear layer maps the output of the bidirectional LSTM into linear layer.\n",
    "        self.linear = nn.Linear(hidden_dim, 128)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "        ### Layer 4: Classifier :\n",
    "        self.hidden2tag = nn.Linear(128, self.tagset_size)\n",
    "    \n",
    "    \n",
    "    def _get_lstm_features(self, sentence):\n",
    "       \n",
    "        ## Loading word embeddings\n",
    "        ## sentence = [sen len, batch, embedding]\n",
    "        embeds = self.word_embeds(sentence)\n",
    "             \n",
    "        ## We concatenate the word embeddings and the character level representation\n",
    "        ## to create unified representation for each word\n",
    "        if len(np.shape(embeds)) == 2:\n",
    "            embeds = embeds.unsqueeze(1)\n",
    "\n",
    "        ## Dropout on the unified embeddings\n",
    "        embeds = self.dropout(embeds)\n",
    "\n",
    "        ## Word lstm\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        ## Reshaping the outputs from the lstm layer\n",
    "        linear_out = self.elu(self.linear(lstm_out)) \n",
    "        \n",
    "        ## Dropout on the lstm output\n",
    "        lstm_out = self.dropout(linear_out)\n",
    "\n",
    "        ## Linear layer converts the ouput vectors to tag space\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "\n",
    "        return lstm_feats\n",
    "    \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        ## returns the LSTM's tag vectors\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        #[sentence length, batch size, output dim]\n",
    "            \n",
    "        return feats\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        ## For evaluation\n",
    "        # features is a 2D tensor, len(sentence) * self.tagset_size\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        #print(np.shape(feats),np.shape(tags))\n",
    "        pred_tags = feats.view(-1, feats.shape[-1])\n",
    "        true_tags = tags.view(-1)\n",
    "        true_tags = Variable(true_tags)\n",
    "        scores = nn.functional.cross_entropy(pred_tags, true_tags)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!!!\n"
     ]
    }
   ],
   "source": [
    "#creating the model using the Class defined above\n",
    "model = BiLSTM(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   pre_word_embeds=word_embeds\n",
    "              )\n",
    "print(\"Model Initialized!!!\")\n",
    "\n",
    "#Initializing the optimizer\n",
    "#The best results in the paper where achived using stochastic gradient descent (SGD) \n",
    "#learning rate=0.015 and momentum=0.9 \n",
    "#decay_rate=0.05 \n",
    "\n",
    "learning_rate = 0.1 #0.015\n",
    "momentum = 0.9\n",
    "number_of_epochs = parameters['epoch'] \n",
    "decay_rate = 0.05\n",
    "gradient_clip = parameters['gradient_clip']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#variables which will used in training process\n",
    "losses = [] #list to store all losses\n",
    "loss = 0.0 #Loss Initializatoin\n",
    "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
    "plot_every = 2000 # Store loss after this many iterations\n",
    "count = 0 #Counts the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import sklearn.metrics\n",
    "def my_evaluating(model, datas):\n",
    "\n",
    "    y_pred=[]\n",
    "    y_true=[]\n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        #print(dwords)\n",
    "        # We are getting the predicted output from our model\n",
    "        if use_gpu:\n",
    "            feats = model(dwords.cuda())\n",
    "        else:\n",
    "            feats = model(dwords)\n",
    "            \n",
    "        feats = feats.view(-1, feats.shape[-1])\n",
    "        _, tag_seq = torch.max(feats, 1)\n",
    "        predicted_id = list(tag_seq.cpu().data)\n",
    "        predicted_id = [i.item() for i in predicted_id]\n",
    "        \n",
    "        y_pred.extend(predicted_id)\n",
    "        y_true.extend(ground_truth_id)\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    report = sklearn.metrics.classification_report(y_true, y_pred, digits=3)\n",
    "    #print(report)\n",
    " \n",
    "    return report\n",
    "#dev_F = my_evaluating(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 2000 : loss = 0.15920651671795286\n",
      "count 4000 : loss = 0.08616005368902224\n",
      "count 6000 : loss = 0.076924574431228\n",
      "count 8000 : loss = 0.06724264426129321\n",
      "count 10000 : loss = 0.08360639633776672\n",
      "count 12000 : loss = 0.04799470001893452\n",
      "count 14000 : loss = 0.052293399750440145\n",
      "count 16000 : loss = 0.03976079923281013\n",
      "count 18000 : loss = 0.02180505887243457\n",
      "count 20000 : loss = 0.024520906471535566\n",
      "count 22000 : loss = 0.02483374579629412\n",
      "count 24000 : loss = 0.027542751486814832\n",
      "count 26000 : loss = 0.024234930539256568\n",
      "count 28000 : loss = 0.01369591030064631\n",
      "count 30000 : loss = 0.022127302217403222\n",
      "count 32000 : loss = 0.014274417325409624\n",
      "count 34000 : loss = 0.010125220593764589\n",
      "count 36000 : loss = 0.007528156209995223\n",
      "count 38000 : loss = 0.013238591375328642\n",
      "count 40000 : loss = 0.009071351863089463\n",
      "count 42000 : loss = 0.0098592696842503\n",
      "count 44000 : loss = 0.00969429588940225\n",
      "count 46000 : loss = 0.0089571128841015\n",
      "count 48000 : loss = 0.0056856959960166105\n",
      "count 50000 : loss = 0.007343848677438733\n",
      "count 52000 : loss = 0.00514303134841793\n",
      "count 54000 : loss = 0.0058637132114422045\n",
      "count 56000 : loss = 0.008761329869591723\n",
      "count 58000 : loss = 0.013508504307218765\n",
      "count 60000 : loss = 0.006335861028775838\n",
      "count 62000 : loss = 0.004952645128943045\n",
      "count 64000 : loss = 0.006041335362374118\n",
      "count 66000 : loss = 0.008685604905403776\n",
      "count 68000 : loss = 0.005618656757311329\n",
      "count 70000 : loss = 0.0048968624129155\n",
      "count 72000 : loss = 0.006126267014641916\n",
      "count 74000 : loss = 0.004320087168568007\n",
      "count 76000 : loss = 0.0026580055585724936\n",
      "count 78000 : loss = 0.0033778656142133233\n",
      "count 80000 : loss = 0.00404574238538277\n",
      "count 82000 : loss = 0.004036150812313946\n",
      "count 84000 : loss = 0.003470460307656829\n",
      "count 86000 : loss = 0.003279027039337335\n",
      "count 88000 : loss = 0.004755941972299136\n",
      "count 90000 : loss = 0.005270500868575731\n",
      "count 92000 : loss = 0.0016039448528650574\n",
      "count 94000 : loss = 0.0028197772806715427\n",
      "count 96000 : loss = 0.001962934047749632\n",
      "count 98000 : loss = 0.00310081042737254\n",
      "count 100000 : loss = 0.003423761214168727\n",
      "count 102000 : loss = 0.003125426385420853\n",
      "count 104000 : loss = 0.0016598075368005197\n",
      "count 106000 : loss = 0.0020895498331017013\n",
      "count 108000 : loss = 0.0037711974714437265\n",
      "count 110000 : loss = 0.0037512352499541766\n",
      "count 112000 : loss = 0.0018476752503672866\n",
      "count 114000 : loss = 0.0025215855124208555\n",
      "count 116000 : loss = 0.0026146277652894607\n",
      "count 118000 : loss = 0.0020486152495725724\n",
      "count 120000 : loss = 0.0028523247480647285\n",
      "count 122000 : loss = 0.001373332148585545\n",
      "count 124000 : loss = 0.0019749421764904134\n",
      "count 126000 : loss = 0.0019079172136075037\n",
      "count 128000 : loss = 0.0010425720445050443\n",
      "count 130000 : loss = 0.0020119716669698526\n",
      "count 132000 : loss = 0.007254770644388176\n",
      "count 134000 : loss = 0.0035706173803457613\n",
      "count 136000 : loss = 0.00215200896121593\n",
      "count 138000 : loss = 0.002988832596918074\n",
      "count 140000 : loss = 0.0015189280361188342\n",
      "count 142000 : loss = 0.0019294984636492763\n",
      "count 144000 : loss = 0.0023365851842963685\n",
      "count 146000 : loss = 0.0026035293850784646\n",
      "count 148000 : loss = 0.0009109423161642661\n",
      "count 150000 : loss = 0.0017146008776281142\n",
      "count 152000 : loss = 0.0015719842319687483\n",
      "count 154000 : loss = 0.0010529436735161417\n",
      "count 156000 : loss = 0.0018576107677365527\n",
      "count 158000 : loss = 0.003999457015367264\n",
      "count 160000 : loss = 0.001088037897650912\n",
      "count 162000 : loss = 0.0021430651146680277\n",
      "count 164000 : loss = 0.0022541660731378163\n",
      "count 166000 : loss = 0.0017424570962240675\n",
      "count 168000 : loss = 0.0006426282241881505\n",
      "count 170000 : loss = 0.0016211931067742706\n",
      "count 172000 : loss = 0.0007448423674065664\n",
      "count 174000 : loss = 0.0007905896492660036\n",
      "count 176000 : loss = 0.0005873876796427972\n",
      "count 178000 : loss = 0.0012084163018233876\n",
      "count 180000 : loss = 0.001184725642963678\n",
      "count 182000 : loss = 0.0015833415255456823\n",
      "count 184000 : loss = 0.0006563360483955755\n",
      "count 186000 : loss = 0.001020514078025392\n",
      "count 188000 : loss = 0.00131058692919132\n",
      "count 190000 : loss = 0.001434085190000815\n",
      "count 192000 : loss = 0.0008141879873766922\n",
      "count 194000 : loss = 0.0005472625004528036\n",
      "count 196000 : loss = 0.0012042269176790075\n",
      "count 198000 : loss = 0.00085895761947044\n",
      "count 200000 : loss = 0.0006101280852256574\n",
      "count 202000 : loss = 0.001074800356871635\n",
      "count 204000 : loss = 0.0009480315831451645\n",
      "count 206000 : loss = 0.0011663849299226924\n",
      "count 208000 : loss = 0.0009473632151446295\n"
     ]
    }
   ],
   "source": [
    "## No batch training\n",
    "count = 0\n",
    "for epoch in range(number_of_epochs):\n",
    "    for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "        count += 1\n",
    "        data = train_data[index]\n",
    "\n",
    "        ##gradient updates for each data entry\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = data['words']\n",
    "        sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "        tags = data['tags']\n",
    "\n",
    "\n",
    "        targets = torch.LongTensor(tags)\n",
    "\n",
    "        #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "        if use_gpu:\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda())\n",
    "        else:\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "        #print(neg_log_likelihood.item())\n",
    "        loss += neg_log_likelihood.item() / len(data['words'])\n",
    "        neg_log_likelihood.backward()\n",
    "\n",
    "        #we use gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        #Storing loss\n",
    "        if count % plot_every == 0:\n",
    "            loss /= plot_every\n",
    "            print(f'count {count} : loss = {loss}')\n",
    "            if losses == []:\n",
    "                losses.append(loss)\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "        #Evaluating on Train, Test, Dev Sets\n",
    "        #if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
    "        #        count % (eval_every*2) == 0 and count < (eval_every * 20):\n",
    "            #dev_report = my_evaluating(model, dev_data)\n",
    "            \n",
    "        #if count >= 20000:  \n",
    "        if count % len(train_data) == 0:\n",
    "            adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_report = my_evaluating(model, dev_data)\n",
    "torch.save(model.state_dict(), 'blstm1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference\n",
    "def my_inference(model, test_data, id_to_tag):\n",
    "    y_pred=[]\n",
    "    for data in test_data:\n",
    "        pre_tag=[]\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        #print(dwords)\n",
    "        # We are getting the predicted output from our model\n",
    "        if use_gpu:\n",
    "            feats = model(dwords.cuda())\n",
    "        else:\n",
    "            feats = model(dwords)\n",
    "\n",
    "        feats = feats.view(-1, feats.shape[-1])\n",
    "        _, tag_seq = torch.max(feats, 1)\n",
    "        predicted_id = list(tag_seq.cpu().data)\n",
    "        predicted_id = [i.item() for i in predicted_id]\n",
    "        for i in predicted_id:\n",
    "            pre_tag.append(id_to_tag[i])\n",
    "        y_pred.append(pre_tag)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test taged datasets\n",
    "y_pred = my_inference(model, dev_data, id_to_tag)\n",
    "#print(y_pred)\n",
    "import functools\n",
    "y_pred= functools.reduce(lambda a, b: a + [''] + b, y_pred)\n",
    "\n",
    "## Output\n",
    "with open('./data/dev', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    #print(len(sentences))\n",
    "    output = []\n",
    "    for i in range(len(sentences)):\n",
    "        line = sentences[i]\n",
    "        if line != '\\n':\n",
    "            line = line.replace('\\n', '') + ' ' + y_pred[i]\n",
    "        output.append(line)\n",
    "        \n",
    "with open('dev1_temp.txt', 'w') as f:\n",
    "    for line in output:\n",
    "        if line != '\\n':\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "        else:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outcome = []\n",
    "for i in range(len(sentences)):\n",
    "    temp = sentences[i].split()\n",
    "    if len(temp) == 3:\n",
    "        temp[2] = y_pred[i]\n",
    "        final_outcome.append(' '.join(temp))\n",
    "    else:\n",
    "        final_outcome.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev1.out.txt', 'w') as f:\n",
    "    for i in final_outcome:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate test2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = my_inference(model, test_data, id_to_tag)\n",
    "#print(y_pred)\n",
    "import functools\n",
    "y_pred_test= functools.reduce(lambda a, b: a + [''] + b, y_pred_test)\n",
    "\n",
    "## Output\n",
    "with open('./data/test', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    output_test = []\n",
    "    for i in range(len(sentences)):\n",
    "        line = sentences[i]\n",
    "        if line != '\\n':\n",
    "            line = line.replace('\\n', '') + ' ' + y_pred_test[i]\n",
    "        output_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test1.out.txt', 'w') as f:\n",
    "    for i in output_test:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! perl conll03eval.txt <  dev1_temp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
