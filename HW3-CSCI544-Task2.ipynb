{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.style.use('seaborn-pastel')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"./data/train\" \n",
    "parameters['dev'] = \"./data/dev\"\n",
    "parameters['test'] = \"./data/test\" \n",
    "parameters['lower'] = True \n",
    "parameters['zeros'] =  False \n",
    "parameters['word_dim'] = 100\n",
    "parameters['word_lstm_dim'] = 256 \n",
    "parameters['word_bidirect'] = True \n",
    "parameters['embedding_path'] = \"./glove.6B.100d\" \n",
    "parameters['dropout'] = 0.33 \n",
    "parameters['epoch'] = 30\n",
    "parameters['gradient_clip']=5.0\n",
    "models_path = \"./models/\" \n",
    "\n",
    "#GPU\n",
    "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
    "use_gpu = parameters['use_gpu']\n",
    "\n",
    "parameters['reload'] = False\n",
    "#Constants\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
    "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
    "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mappings for Words and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[1].lower() if lower else x[1] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21011 unique words (204567 in total)\n",
      "Found 11 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function prepare dataset returns a list of dictionaries ( one dictionary per each sentence )\n",
    "\n",
    "Each of the dictionary returned by the function contains\n",
    "   1. list of all words in the sentence\n",
    "   2. list of word index for all words in the sentence\n",
    "   3. list of lists, containing character id of each character for words in the sentence\n",
    "   4. list of tag for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14987 / 3466 / 3684 sentences in train / dev / train.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(sentences, word_to_id, tag_to_id, lower=False, test=0):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[1] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        if test == 0:\n",
    "            tags = [tag_to_id[w[-1]] for w in s]\n",
    "            data.append({\n",
    "                'str_words': str_words,\n",
    "                'words': words,\n",
    "                #'chars': chars,\n",
    "                'tags': tags,\n",
    "            })\n",
    "        else:\n",
    "            data.append({\n",
    "                'str_words': str_words,\n",
    "                'words': words,\n",
    "            })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, tag_to_id, parameters['lower'], test = 1\n",
    ")\n",
    "print(\"{} / {} / {} sentences in train / dev / train.\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Embeddings\n",
    "\n",
    "Now, We will load the pre-trained word embeddings glove.6B.100d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == parameters['word_dim'] + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "#Intializing Word Embedding Matrix\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
    "\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
    "                  pre_word_embeds=None, use_gpu=False):\n",
    "        \n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        #parameter initialization for the model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        \n",
    "        ### Layer 1: Embedding :\n",
    "        \n",
    "        #Word Embedding\n",
    "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            #Initializes the word embeddings with pretrained word embeddings\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "        \n",
    "        #Initializing the dropout layer, with dropout specificed in parameters\n",
    "        self.dropout = nn.Dropout(parameters['dropout'])\n",
    "        \n",
    "        ### Layer 2: Lstm :\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim // 2, \n",
    "                            num_layers=1,\n",
    "                            bidirectional=True,\n",
    "                            )      #dropout=0 \n",
    "\n",
    "        \n",
    "        ### Layer 3: Linear + ELU\n",
    "        #Linear layer maps the output of the bidirectional LSTM into linear layer.\n",
    "        self.linear = nn.Linear(hidden_dim, 128)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "        ### Layer 4: Classifier :\n",
    "        self.hidden2tag = nn.Linear(128, self.tagset_size)\n",
    "    \n",
    "    def _get_lstm_features(self, sentence):\n",
    "       \n",
    "        ## Loading word embeddings\n",
    "        ## sentence = [sen len, batch, embedding]\n",
    "        embeds = self.word_embeds(sentence)\n",
    "             \n",
    "        ## We concatenate the word embeddings and the character level representation\n",
    "        ## to create unified representation for each word\n",
    "        if len(np.shape(embeds)) == 2:\n",
    "            embeds = embeds.unsqueeze(1)\n",
    "\n",
    "        ## Dropout on the unified embeddings\n",
    "        embeds = self.dropout(embeds)\n",
    "\n",
    "        ## Word lstm\n",
    "        ## Takes words as input and generates a output at each step\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "        ## Reshaping the outputs from the lstm layer\n",
    "        linear_out = self.elu(self.linear(lstm_out)) \n",
    "        \n",
    "        ## Dropout on the lstm output\n",
    "        lstm_out = self.dropout(linear_out)\n",
    "\n",
    "        ## Linear layer converts the ouput vectors to tag space\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        ## returns the LSTM's tag vectors\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        #[sentence length, batch size, output dim]\n",
    "            \n",
    "        return feats\n",
    "    \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        ## For evaluation\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        #print(np.shape(feats),np.shape(tags))\n",
    "        pred_tags = feats.view(-1, feats.shape[-1])\n",
    "        true_tags = tags.view(-1)\n",
    "        true_tags = Variable(true_tags)\n",
    "        #scores = nn.functional.cross_entropy(pred_tags, true_tags, ignore_index=11)\n",
    "        scores = nn.functional.cross_entropy(pred_tags, true_tags)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!!!\n"
     ]
    }
   ],
   "source": [
    "#creating the model using the Class defined above\n",
    "model = BiLSTM(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   #char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds\n",
    "                   #char_mode=parameters['char_mode']\n",
    "              )\n",
    "print(\"Model Initialized!!!\")\n",
    "\n",
    "\n",
    "learning_rate = 0.02 #0.015\n",
    "momentum = 0.9\n",
    "number_of_epochs = parameters['epoch'] \n",
    "decay_rate = 0.05\n",
    "gradient_clip = parameters['gradient_clip']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#variables which will used in training process\n",
    "losses = [] #list to store all losses\n",
    "loss = 0.0 #Loss Initializatoin\n",
    "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
    "plot_every = 2000 # Store loss after this many iterations\n",
    "count = 0 #Counts the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import sklearn.metrics\n",
    "def my_evaluating(model, datas):\n",
    "\n",
    "    y_pred=[]\n",
    "    y_true=[]\n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        #print(dwords)\n",
    "        # We are getting the predicted output from our model\n",
    "        if use_gpu:\n",
    "            feats = model(dwords.cuda())\n",
    "        else:\n",
    "            feats = model(dwords)\n",
    "            \n",
    "        feats = feats.view(-1, feats.shape[-1])\n",
    "        _, tag_seq = torch.max(feats, 1)\n",
    "        predicted_id = list(tag_seq.cpu().data)\n",
    "        predicted_id = [i.item() for i in predicted_id]\n",
    "        \n",
    "        y_pred.extend(predicted_id)\n",
    "        y_true.extend(ground_truth_id)\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    #non_pad_elements = (y_true != 11).nonzero()\n",
    "    ##correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "\n",
    "    #y_pred = y_pred[non_pad_elements]\n",
    "    #y_true = y_true[non_pad_elements]\n",
    "    report = sklearn.metrics.classification_report(y_true, y_pred, digits=3)\n",
    "    #plot(report)\n",
    " \n",
    "    return report\n",
    "#dev_F = my_evaluating(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 2000 : loss = 0.09152249355068356\n",
      "count 4000 : loss = 0.06424437796717132\n",
      "count 6000 : loss = 0.03237820543599763\n",
      "count 8000 : loss = 0.038240996368043664\n",
      "count 10000 : loss = 0.030437080464281525\n",
      "count 12000 : loss = 0.0335558607627015\n",
      "count 14000 : loss = 0.023227903596272696\n",
      "count 16000 : loss = 0.016989820983909627\n",
      "count 18000 : loss = 0.018023057424901408\n",
      "count 20000 : loss = 0.019737876486324656\n",
      "count 22000 : loss = 0.016220923745540156\n",
      "count 24000 : loss = 0.01731514153096755\n",
      "count 26000 : loss = 0.023509941263050005\n",
      "count 28000 : loss = 0.01821931910037259\n",
      "count 30000 : loss = 0.01968868960241735\n",
      "count 32000 : loss = 0.012898035684758124\n",
      "count 34000 : loss = 0.01323172734481975\n",
      "count 36000 : loss = 0.011504615182542222\n",
      "count 38000 : loss = 0.008875840609788518\n",
      "count 40000 : loss = 0.014793122507304638\n",
      "count 42000 : loss = 0.011412374068283394\n",
      "count 44000 : loss = 0.01062500152946601\n",
      "count 46000 : loss = 0.009705541310828814\n",
      "count 48000 : loss = 0.010272886570429367\n",
      "count 50000 : loss = 0.009145290246235347\n",
      "count 52000 : loss = 0.00781122157485344\n",
      "count 54000 : loss = 0.011908433203206361\n",
      "count 56000 : loss = 0.008927402565638934\n",
      "count 58000 : loss = 0.007454582957034924\n",
      "count 60000 : loss = 0.009369138494998559\n",
      "count 62000 : loss = 0.005685010751402023\n",
      "count 64000 : loss = 0.00780482509819759\n",
      "count 66000 : loss = 0.004740342160031432\n",
      "count 68000 : loss = 0.008203398091742857\n",
      "count 70000 : loss = 0.006117845956447889\n",
      "count 72000 : loss = 0.007859095161627055\n",
      "count 74000 : loss = 0.006412175671251043\n",
      "count 76000 : loss = 0.0056790278724028675\n",
      "count 78000 : loss = 0.0063842097459601375\n",
      "count 80000 : loss = 0.004183516503750595\n",
      "count 82000 : loss = 0.007098871655933007\n",
      "count 84000 : loss = 0.006604155876546876\n",
      "count 86000 : loss = 0.0061072348082111365\n",
      "count 88000 : loss = 0.004481926106601664\n",
      "count 90000 : loss = 0.006493475569736036\n",
      "count 92000 : loss = 0.0053957097949639855\n",
      "count 94000 : loss = 0.004536248443307065\n",
      "count 96000 : loss = 0.0045433535319421405\n",
      "count 98000 : loss = 0.0048740866302229785\n",
      "count 100000 : loss = 0.004769118531476678\n",
      "count 102000 : loss = 0.005636316403177477\n",
      "count 104000 : loss = 0.0042131463924155186\n",
      "count 106000 : loss = 0.0040458298914360265\n",
      "count 108000 : loss = 0.004385192917402591\n",
      "count 110000 : loss = 0.0052974539249866556\n",
      "count 112000 : loss = 0.004579181654140873\n",
      "count 114000 : loss = 0.004553416151519048\n",
      "count 116000 : loss = 0.003959922326801433\n",
      "count 118000 : loss = 0.003677447841113665\n",
      "count 120000 : loss = 0.003709335244843868\n",
      "count 122000 : loss = 0.004374871592832603\n",
      "count 124000 : loss = 0.0036700079054042415\n",
      "count 126000 : loss = 0.0035058171660391317\n",
      "count 128000 : loss = 0.003254077520748258\n",
      "count 130000 : loss = 0.003366708491835346\n",
      "count 132000 : loss = 0.004369089232139094\n",
      "count 134000 : loss = 0.0036245765924385206\n",
      "count 136000 : loss = 0.0033618118358699728\n",
      "count 138000 : loss = 0.002722020693006893\n",
      "count 140000 : loss = 0.0035867302436996063\n",
      "count 142000 : loss = 0.003454807335390101\n",
      "count 144000 : loss = 0.0037131107405763953\n",
      "count 146000 : loss = 0.0035862162184383778\n",
      "count 148000 : loss = 0.003537739131450234\n",
      "count 150000 : loss = 0.0022514728333350603\n",
      "count 152000 : loss = 0.002646535349375304\n",
      "count 154000 : loss = 0.0029236928550947917\n",
      "count 156000 : loss = 0.0028758435256638643\n",
      "count 158000 : loss = 0.0023973459931733642\n",
      "count 160000 : loss = 0.003847991606041899\n",
      "count 162000 : loss = 0.0032938834781088937\n",
      "count 164000 : loss = 0.003113813086639384\n",
      "count 166000 : loss = 0.003450168388272407\n",
      "count 168000 : loss = 0.0027070679809239814\n",
      "count 170000 : loss = 0.004830717287536512\n",
      "count 172000 : loss = 0.0027308016181008094\n",
      "count 174000 : loss = 0.003268012928470779\n",
      "count 176000 : loss = 0.0025264188862111096\n",
      "count 178000 : loss = 0.002567505528769653\n",
      "count 180000 : loss = 0.003553582988180754\n",
      "count 182000 : loss = 0.0023623398173098806\n",
      "count 184000 : loss = 0.002049184331894685\n",
      "count 186000 : loss = 0.0021768685903307237\n",
      "count 188000 : loss = 0.002083218667953539\n",
      "count 190000 : loss = 0.0016986583860647004\n",
      "count 192000 : loss = 0.003028482010969126\n",
      "count 194000 : loss = 0.002115197703837088\n",
      "count 196000 : loss = 0.0029041331539439524\n",
      "count 198000 : loss = 0.002080708651567023\n",
      "count 200000 : loss = 0.0030518001741538375\n",
      "count 202000 : loss = 0.00323028886027432\n",
      "count 204000 : loss = 0.002023646672236025\n",
      "count 206000 : loss = 0.0018484545500335454\n",
      "count 208000 : loss = 0.002229289545943543\n",
      "count 210000 : loss = 0.0019160877695422773\n",
      "count 212000 : loss = 0.0017642087429814773\n",
      "count 214000 : loss = 0.0029527667003263524\n",
      "count 216000 : loss = 0.0016830258284437577\n",
      "count 218000 : loss = 0.0025693786267961334\n",
      "count 220000 : loss = 0.00219148628754086\n",
      "count 222000 : loss = 0.002375169516258559\n",
      "count 224000 : loss = 0.0025392289593104338\n",
      "count 226000 : loss = 0.0019988902907184867\n",
      "count 228000 : loss = 0.002155153747506255\n",
      "count 230000 : loss = 0.001578763029621448\n",
      "count 232000 : loss = 0.0016293092299330754\n",
      "count 234000 : loss = 0.0015893809462530918\n",
      "count 236000 : loss = 0.001810905049672794\n",
      "count 238000 : loss = 0.0022779370793747753\n",
      "count 240000 : loss = 0.001603298143908495\n",
      "count 242000 : loss = 0.0018227850340975156\n",
      "count 244000 : loss = 0.0016200693929123377\n",
      "count 246000 : loss = 0.0015500951816611501\n",
      "count 248000 : loss = 0.0018373593167710933\n",
      "count 250000 : loss = 0.0026358517475817637\n",
      "count 252000 : loss = 0.0015671436893376568\n",
      "count 254000 : loss = 0.0015855874591273924\n",
      "count 256000 : loss = 0.0022426933045463396\n",
      "count 258000 : loss = 0.0014054724196147037\n",
      "count 260000 : loss = 0.0013028456820080604\n",
      "count 262000 : loss = 0.002579102284356153\n",
      "count 264000 : loss = 0.0018107466519106951\n",
      "count 266000 : loss = 0.0014923654909417656\n",
      "count 268000 : loss = 0.0015122076051480287\n",
      "count 270000 : loss = 0.001314524587184313\n",
      "count 272000 : loss = 0.0019468415673734635\n",
      "count 274000 : loss = 0.0012253721722372513\n",
      "count 276000 : loss = 0.001686119389576795\n",
      "count 278000 : loss = 0.0016264500531084304\n",
      "count 280000 : loss = 0.0011475021421647226\n",
      "count 282000 : loss = 0.0020579283685538377\n",
      "count 284000 : loss = 0.0018773190631602376\n",
      "count 286000 : loss = 0.0016997565580435198\n",
      "count 288000 : loss = 0.0020306923002621664\n",
      "count 290000 : loss = 0.0016669744681670096\n",
      "count 292000 : loss = 0.0015925672215076432\n",
      "count 294000 : loss = 0.0011334818757810555\n",
      "count 296000 : loss = 0.0017926312210738708\n",
      "count 298000 : loss = 0.00158195095554735\n",
      "count 300000 : loss = 0.002094143653860613\n",
      "count 302000 : loss = 0.0011366750644918493\n",
      "count 304000 : loss = 0.0009378364445961221\n",
      "count 306000 : loss = 0.0009205635380852318\n",
      "count 308000 : loss = 0.0013669690347087498\n",
      "count 310000 : loss = 0.0015764408007714963\n",
      "count 312000 : loss = 0.0012194968425632779\n",
      "count 314000 : loss = 0.001155294551417782\n",
      "count 316000 : loss = 0.000998751350862876\n",
      "count 318000 : loss = 0.0007983940159761946\n",
      "count 320000 : loss = 0.0012520805274292197\n",
      "count 322000 : loss = 0.0011457638932531313\n",
      "count 324000 : loss = 0.0012048445131833984\n",
      "count 326000 : loss = 0.0010929387539273705\n",
      "count 328000 : loss = 0.0012709979464214061\n",
      "count 330000 : loss = 0.0010687447791128057\n",
      "count 332000 : loss = 0.0010033414048523119\n",
      "count 334000 : loss = 0.0007921625554233407\n",
      "count 336000 : loss = 0.0010681880643517272\n",
      "count 338000 : loss = 0.0008154198887539135\n",
      "count 340000 : loss = 0.0014765805967676211\n",
      "count 342000 : loss = 0.0006646021427915956\n",
      "count 344000 : loss = 0.0022026596284618898\n",
      "count 346000 : loss = 0.0011065991986598837\n",
      "count 348000 : loss = 0.0011575629076487853\n",
      "count 350000 : loss = 0.0009921244969166312\n",
      "count 352000 : loss = 0.0019187184756977536\n",
      "count 354000 : loss = 0.0011113588843245418\n",
      "count 356000 : loss = 0.0012186212332092386\n",
      "count 358000 : loss = 0.0009210339249687644\n",
      "count 360000 : loss = 0.0010861168250013095\n",
      "count 362000 : loss = 0.0007386258192417443\n",
      "count 364000 : loss = 0.0007132467444792369\n",
      "count 366000 : loss = 0.0006815708083019833\n",
      "count 368000 : loss = 0.0012696331314867553\n",
      "count 370000 : loss = 0.001423641636525838\n",
      "count 372000 : loss = 0.0008026747806991597\n",
      "count 374000 : loss = 0.0008418347641328518\n",
      "count 376000 : loss = 0.001120988617642569\n",
      "count 378000 : loss = 0.0007547904212177053\n",
      "count 380000 : loss = 0.000559562437729013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 382000 : loss = 0.0010513259365643654\n",
      "count 384000 : loss = 0.0013622255544210359\n",
      "count 386000 : loss = 0.0009821351311729279\n",
      "count 388000 : loss = 0.0009002120293498726\n",
      "count 390000 : loss = 0.0010537606140795463\n",
      "count 392000 : loss = 0.001124367525977347\n",
      "count 394000 : loss = 0.0007592606509680205\n",
      "count 396000 : loss = 0.0008145382914119287\n",
      "count 398000 : loss = 0.0009182712143610487\n",
      "count 400000 : loss = 0.0007275289438185533\n",
      "count 402000 : loss = 0.0010000532094373684\n",
      "count 404000 : loss = 0.0006257779501508328\n",
      "count 406000 : loss = 0.0006646887165139161\n",
      "count 408000 : loss = 0.0010723858803745913\n",
      "count 410000 : loss = 0.0010549789505972173\n",
      "count 412000 : loss = 0.0007879071746858132\n",
      "count 414000 : loss = 0.0007269492248245538\n",
      "count 416000 : loss = 0.0005503192797608081\n",
      "count 418000 : loss = 0.0010625018588633934\n",
      "count 420000 : loss = 0.0008855646499767383\n",
      "count 422000 : loss = 0.0007020527943426048\n",
      "count 424000 : loss = 0.0008002630233595629\n",
      "count 426000 : loss = 0.0005722654806102687\n",
      "count 428000 : loss = 0.0005935573448942221\n",
      "count 430000 : loss = 0.0005538043599575296\n",
      "count 432000 : loss = 0.0007474628342877348\n",
      "count 434000 : loss = 0.0007803344076573952\n",
      "count 436000 : loss = 0.000922834830771513\n",
      "count 438000 : loss = 0.0013665169763697415\n",
      "count 440000 : loss = 0.0007387182311256311\n",
      "count 442000 : loss = 0.0008696570907201884\n",
      "count 444000 : loss = 0.0007146117766622087\n",
      "count 446000 : loss = 0.0007372598063872576\n",
      "count 448000 : loss = 0.001530557418741672\n"
     ]
    }
   ],
   "source": [
    "## No batch training\n",
    "count = 0\n",
    "for epoch in range(number_of_epochs):\n",
    "    for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "        count += 1\n",
    "        data = train_data[index]\n",
    "\n",
    "        ##gradient updates for each data entry\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = data['words']\n",
    "        sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "        tags = data['tags']\n",
    "\n",
    "\n",
    "        targets = torch.LongTensor(tags)\n",
    "\n",
    "        #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "        if use_gpu:\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda())\n",
    "        else:\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "        #print(neg_log_likelihood.item())\n",
    "        loss += neg_log_likelihood.item() / len(data['words'])\n",
    "        neg_log_likelihood.backward()\n",
    "\n",
    "        #we use gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        #Storing loss\n",
    "        if count % plot_every == 0:\n",
    "            loss /= plot_every\n",
    "            print(f'count {count} : loss = {loss}')\n",
    "            if losses == []:\n",
    "                losses.append(loss)\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "        #Evaluating on Train, Test, Dev Sets\n",
    "        if count % (eval_every*2) == 0 and count > (eval_every * 20) or \\\n",
    "                count % (eval_every*5) == 0 and count < (eval_every * 20):\n",
    "            dev_report = my_evaluating(model, dev_data)\n",
    "        # if count % len(train_data) == 0:\n",
    "        if (count >= 20*len(train_data)) and (count % (4*len(train_data)) == 0) :     \n",
    "            adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_report = my_evaluating(model, dev_data)\n",
    "torch.save(model.state_dict(), 'blstm2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dev2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference\n",
    "def my_inference(model, test_data, id_to_tag):\n",
    "    y_pred=[]\n",
    "    for data in test_data:\n",
    "        pre_tag=[]\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        if use_gpu:\n",
    "            feats = model(dwords.cuda())\n",
    "        else:\n",
    "            feats = model(dwords)\n",
    "\n",
    "        feats = feats.view(-1, feats.shape[-1])\n",
    "        _, tag_seq = torch.max(feats, 1)\n",
    "        predicted_id = list(tag_seq.cpu().data)\n",
    "        predicted_id = [i.item() for i in predicted_id]\n",
    "        for i in predicted_id:\n",
    "            pre_tag.append(id_to_tag[i])\n",
    "        y_pred.append(pre_tag)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test taged datasets\n",
    "y_pred = my_inference(model, dev_data, id_to_tag)\n",
    "#print(y_pred)\n",
    "import functools\n",
    "y_pred= functools.reduce(lambda a, b: a + [''] + b, y_pred)\n",
    "\n",
    "## Output\n",
    "with open('./data/dev', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    output = []\n",
    "    for i in range(len(sentences)):\n",
    "        line = sentences[i]\n",
    "        if line != '\\n':\n",
    "            line = line.replace('\\n', '') + ' ' + y_pred[i]\n",
    "        output.append(line)\n",
    "        \n",
    "with open('dev2_temp.txt', 'w') as f:\n",
    "    for line in output:\n",
    "        if line != '\\n':\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "        else:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outcome = []\n",
    "for i in range(len(sentences)):\n",
    "    temp = sentences[i].split()\n",
    "    if len(temp) == 3:\n",
    "        temp[2] = y_pred[i]\n",
    "        final_outcome.append(' '.join(temp))\n",
    "    else:\n",
    "        final_outcome.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev2.out.txt', 'w') as f:\n",
    "    for i in final_outcome:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate test2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = my_inference(model, test_data, id_to_tag)\n",
    "#print(y_pred)\n",
    "import functools\n",
    "y_pred_test= functools.reduce(lambda a, b: a + [''] + b, y_pred_test)\n",
    "\n",
    "## Output\n",
    "with open('./data/test', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    output_test = []\n",
    "    for i in range(len(sentences)):\n",
    "        line = sentences[i]\n",
    "        if line != '\\n':\n",
    "            line = line.replace('\\n', '') + ' ' + y_pred_test[i]\n",
    "        output_test.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test2.out.txt', 'w') as f:\n",
    "    for i in output_test:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5582 phrases; correct: 4761.\r\n",
      "accuracy:  96.28%; precision:  85.29%; recall:  80.12%; FB1:  82.63\r\n",
      "              LOC: precision:  93.05%; recall:  88.24%; FB1:  90.58  1742\r\n",
      "             MISC: precision:  78.59%; recall:  74.84%; FB1:  76.67  878\r\n",
      "              ORG: precision:  74.01%; recall:  75.39%; FB1:  74.70  1366\r\n",
      "              PER: precision:  90.16%; recall:  78.12%; FB1:  83.71  1596\r\n"
     ]
    }
   ],
   "source": [
    "! perl conll03eval.txt <  dev2_temp_new.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
