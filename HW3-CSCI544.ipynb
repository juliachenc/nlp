{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"dev\", sep = \"\\t\", index_col=False, header=None)\n",
    "dev.columns = [\"index\", \"word\", \"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train\", sep = \"\\t\", index_col=False, header=None)\n",
    "train.columns = [\"index\", \"word\", \"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test\", sep = \"\\t\", index_col=False, header=None)\n",
    "test.columns = [\"index\", \"word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique vocab\n",
    "countVoc = train['word'].value_counts().rename_axis('word').reset_index(name='counts')\n",
    "# less freq part\n",
    "lessfreq = countVoc.loc[countVoc['counts'] < 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the word as '<unk>' if the frequence less than 3 \n",
    "countVoc.loc[countVoc['counts'] < 3, 'word'] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the summation counts where the words are the same,\n",
    "# the only case where words are different is '<unk>'\n",
    "countVoc = countVoc.groupby(['word']).sum('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort thhe vocabulary by its count\n",
    "countVoc = countVoc.sort_values(by=['counts'], ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# move 'unk' to the first line\n",
    "unk_word = countVoc[countVoc['word']=='<unk>']\n",
    "countVoc = countVoc[countVoc['word']!='<unk>'].reset_index()\n",
    "vocab = pd.concat([unk_word, countVoc], ignore_index = True, axis = 0)\n",
    "vocab = vocab.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.concat([unk_word, countVoc], ignore_index = True, axis = 0)\n",
    "vocab = vocab.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>32537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  counts\n",
       "0  <unk>   32537"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[vocab['word']=='<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected threshold for unknown vocabulary is 2, so the size of my \n",
    "vocabulary is 16920, the occurrences of token '< unk >' after replacement are 32537."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab .to_csv('/Users/juliachen/Desktop/CSCI 544/homework/hw3/vocab.txt',\n",
    "#              header = False,index=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing model learning, I first of all replace all less frequence work as '< unk >', and consider the there is a posssible situation that an unknow word will be appear as teh first word, so I also include it in the ts1, which is the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train\n",
    "train2.loc[train2['word'].isin(lessfreq['word']), 'word'] = '<unk>'\n",
    "# train2.loc[(train2['word'] == '<unk>'), 'tag'] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the initial state\n",
    "firstSent = train2[train2['index']==1]\n",
    "ts1 = dict(firstSent['tag'].value_counts())\n",
    "for key, val in ts1.items():\n",
    "    ts1[key] = val / len(firstSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unkFirst = firstSent[firstSent['word']=='<unk>']\n",
    "ts1['<unk>'] = len(unkFirst)/len(firstSent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition parameters for HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find unique state as a dictionary\n",
    "2. Creat empty dictionary for count(s->s')\n",
    "3. For each sentence, go over all tags \n",
    "4. Calculate the transition by count(s->s')/count(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate s\n",
    "count_tags= train['tag'].value_counts().rename_axis('tag').reset_index(name='count')\n",
    "tags = count_tags.set_index('tag').to_dict()['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dict for transition\n",
    "tags_tran = {}\n",
    "transition = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate s->s'\n",
    "for key, val in tags.items():\n",
    "    tags_tran[key] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate next state s' for each s\n",
    "tag_list = train['tag'].to_list()\n",
    "\n",
    "for i in range(len(tag_list)-1):\n",
    "    s = tag_list[i]\n",
    "    s_prime = tag_list[i+1]\n",
    "    tags_tran[s][s_prime] = tags_tran[s].get(s_prime, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition \n",
    "for key, val in tags_tran.items():\n",
    "    count = tags[key]\n",
    "    for key2, val2 in val.items():\n",
    "        total = val2\n",
    "        transition[(key, key2)] = total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = dict((','.join(k), v) for k,v in transition.items())\n",
    "#with open('/Users/juliachen/Desktop/CSCI 544/homework/hw3/transition.json', 'w') as fp:\n",
    "#    json.dump(data, fp, indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission parameters for HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find unique state as a dictionary\n",
    "2. Creat empty dictionary for count(s->x)\n",
    "3. For each sentence, go over all word\n",
    "4. Calculate the transition by count(s->x)/count(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dict for emission transition\n",
    "emm_tran = {}\n",
    "emission = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate s->x\n",
    "for key, val in tags.items():\n",
    "    emm_tran[key] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate next state x for each s\n",
    "word_list = train['word'].to_list()\n",
    "tag_list = train['tag'].to_list() \n",
    "\n",
    "for i in range(len(word_list)):\n",
    "    x = word_list[i]\n",
    "    s = tag_list[i]\n",
    "    emm_tran[s][x] = emm_tran[s].get(x, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# emission  \n",
    "for key, val in emm_tran.items():\n",
    "    count = tags[key]\n",
    "    for key2, val2 in val.items():\n",
    "        total = val2\n",
    "        #print(key2, \":\", total)\n",
    "        emission[(key, key2)] = total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm = {}\n",
    "# hmm['transition'] = transition\n",
    "# hmm['emission'] = emission\n",
    "\n",
    "# data = dict((','.join(k), v) for k,v in emission.items())\n",
    "# with open('/Users/juliachen/Desktop/CSCI 544/homework/hw3/hmm.json', 'w') as fp:\n",
    "#     json.dump(data, fp, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1378 transition parameters, and 23373 emission parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(transition), \"transition parameters, and\", len(emission), \"emission parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1378 transition parameters and 23373 emission parameters in my HMM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Greedy Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev', 'r') as fp:\n",
    "    sentence = []\n",
    "    dev_sentences = []\n",
    "    for line in fp:\n",
    "        line = line.replace('\\n', '').split('\\t')\n",
    "        #print(len(line))\n",
    "        if len(line) == 1:\n",
    "            dev_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test', 'r') as fp:\n",
    "    sentence = []\n",
    "    test_sentences = []\n",
    "    for line in fp:\n",
    "        line = line.replace('\\n', '').split('\\t')\n",
    "        #print(len(line))\n",
    "        if len(line) == 1:\n",
    "            test_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition(previous_tag):\n",
    "    # tags_list = {}\n",
    "    tags_list = []\n",
    "    for tag, val in transition.items():\n",
    "        if tag[0] == previous_tag:\n",
    "            tags_list.append((tag[1], val))\n",
    "            #tags_list.update({tag[1]: val})\n",
    "            \n",
    "    return tags_list\n",
    "\n",
    "# get_transition('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission(word):\n",
    "    # tags_list ={}\n",
    "    tags_list =[]\n",
    "    if word in vocab['word'].to_list():\n",
    "        for tag, val in emission.items():\n",
    "            if tag[1] == word:\n",
    "                #tags_list.update({tag[0]: val})\n",
    "                tags_list.append((tag[0], val))\n",
    "                \n",
    "    else: \n",
    "        for tag, val in emission.items():\n",
    "            if tag[1] == '<unk>':\n",
    "                # tags_list.update({tag[0]: val})\n",
    "                tags_list.append((tag[0], val))\n",
    "                \n",
    "    return tags_list\n",
    "\n",
    "#get_emission('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s*1 = arg max t(s1)e(x1|s1)\n",
    "def s1(word):\n",
    "    highest = 0\n",
    "    for tag, prob in word:\n",
    "        if tag not in ts1:\n",
    "            score = prob*ts1['<unk>']\n",
    "        else:\n",
    "            score = prob*ts1[tag]\n",
    "        if score >= highest:\n",
    "            highest = score\n",
    "            ps1 = tag\n",
    "    return ps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s*2 = arg max t(s2|s*1)e(x2|s2)\n",
    "def s2(word, tag):\n",
    "    highest = -1\n",
    "    for key1, val1 in word:\n",
    "        for key2, val2 in tag:\n",
    "            if key1 == key2:\n",
    "                score = val1*val2\n",
    "                if score >= highest:\n",
    "                    highest = score\n",
    "                    ps2 = key1\n",
    "    return ps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuaracy\n",
    "def acc(pred, label):\n",
    "    count = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == label[i]:\n",
    "            count = count + 1\n",
    "        else:\n",
    "            continue\n",
    "    return((count)/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy \n",
    "def greed_model(sentence):\n",
    "    labels = []\n",
    "    preds = []\n",
    "    \n",
    "    for item in sentence:\n",
    "        \n",
    "        idx = item[0]\n",
    "        word = item[1]\n",
    "        tag = item[2]\n",
    "        labels.append(tag)\n",
    "        if idx == '1':\n",
    "            curr_words = get_emission(word)\n",
    "            prev_tag = s1(curr_words)\n",
    "            predictions.append(prev_tag)\n",
    "            \n",
    "        else:\n",
    "            all_words = get_emission(word)\n",
    "            curr_tags = get_transition(prev_tag)\n",
    "            try: \n",
    "                ps2 = s2(all_words, curr_tags)\n",
    "                \n",
    "            except UnboundLocalError:\n",
    "                # ps2 = max(all_words)[0][0]\n",
    "                ps2 = sorted(all_words, key = lambda x: x[1], reverse = True)[0][0]\n",
    "            prev_tag = ps2\n",
    "            preds.append(ps2)\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions=[]; labels=[]\n",
    "for sentence in dev_sentences:\n",
    "    prediction, label = greed_model(sentence)\n",
    "    predictions.extend(prediction)\n",
    "    labels.extend(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9300726370198329"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(predictions,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greed_model_test(sentence):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for item in sentence:\n",
    "    \n",
    "        if item[0] == '1':\n",
    "            potential_words = get_emission(item[1])\n",
    "            ps1 = s1(potential_words)\n",
    "            previous_tag = ps1\n",
    "            predictions.append(ps1)\n",
    "            \n",
    "        else: \n",
    "            potential_words2 = get_emission(item[1])\n",
    "            potential_tags = get_transition(previous_tag)\n",
    "            try: \n",
    "                ps2 = s2(potential_words2,potential_tags)\n",
    "            except UnboundLocalError:\n",
    "                ps2 = sorted(potential_words2, key = lambda x: x[1], reverse = True)[0][0]\n",
    "                \n",
    "            previous_tag = ps2\n",
    "            predictions.append(ps2)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test=[]\n",
    "for sentence in test_sentences:\n",
    "    prediction= greed_model_test(sentence)\n",
    "    predictions_test.extend(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('greedy.out.txt', 'w') as f:\n",
    "    for i in range(len(predictions_test)):\n",
    "        f.write(str(test['index'][i]))\n",
    "        f.write('\\t')\n",
    "        f.write(str(test['word'][i]))\n",
    "        f.write('\\t')\n",
    "        f.write(predictions_test[i])\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state(potential_words):\n",
    "    for tag, prob in potential_words:\n",
    "        if tag not in ts1:\n",
    "            continue\n",
    "        ts = ts1[tag]\n",
    "        dp[0][tag] = ts*prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_valid(dp, index):\n",
    "    total_score = 0\n",
    "    for key, val in dp[index].items():\n",
    "        total_score += val['cur_best_score']\n",
    "    if total_score == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_forward(sentence, dp):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    sentence_len = 0\n",
    "    for element in sentence:\n",
    "        sentence_len += 1\n",
    "        index = int(element[0]) - 1\n",
    "        word = element[1]\n",
    "        label_tag = element[2]\n",
    "        labels.append(label_tag)\n",
    "        if index == 0:\n",
    "            initial_word = word\n",
    "            potential_words = get_emission(ts1)\n",
    "            get_initial_state(potential_words)\n",
    "            continue\n",
    "            \n",
    "        potential_words2 = get_emission(word)\n",
    "        for tag, e_x_s in potential_words2: #prob is e(x|s2)\n",
    "            best_score1 = 0 #record the best ps1\n",
    "            best_s1_for_s2 = None\n",
    "            if index == 1:\n",
    "                for previous_tag, ps1 in dp[0].items(): #val is dp[i-1][s1]\n",
    "                    try:\n",
    "                        t_s2_s1 = transition[(tag, previous_tag)] #val2 is t(s2|s1\n",
    "                    except:\n",
    "                        t_s2_s1 = 0\n",
    "                    if ps1*t_s2_s1*e_x_s > best_score1:\n",
    "                        best_score1 = ps1*t_s2_s1*e_x_s\n",
    "                        best_s1_for_s2 = previous_tag\n",
    "                        dp[index][tag] = {'cur_best_score':best_score1,\n",
    "                                          'from': best_s1_for_s2}\n",
    "            else:\n",
    "                for previous_tag, d in dp[index-1].items():\n",
    "                    ps1 = d['cur_best_score']\n",
    "                try:\n",
    "                    t_s2_s1 = transition[(tag, previous_tag)]\n",
    "                except:\n",
    "                    t_s2_s1 = 0\n",
    "                if ps1*t_s2_s1*e_x_s > best_score1:\n",
    "                    best_score1 = ps1*t_s2_s1*e_x_s\n",
    "                    best_s1_for_s2 = previous_tag\n",
    "                    dp[index][tag] = {'cur_best_score':best_score1,\n",
    "                                      'from': best_s1_for_s2}\n",
    "        if not_valid(dp, index):\n",
    "            tag = sorted(potential_words2, key=lambda x: x[1], reverse=True)[0][0]\n",
    "            #tag = potential_words2.sort()\n",
    "            best_score2 = 0\n",
    "            for key, val in dp[index-1].items():\n",
    "                cur_score2 = val['cur_best_score']\n",
    "                if cur_score2 > best_score2:\n",
    "                    best_s1_for_s2 = key\n",
    "                    best_score2 = cur_score2\n",
    "            dp[index][tag] = {'cur_best_score':best_score2, 'from': best_s1_for_s2}\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "def viterbi_backward(dp, index, from_, prediction):\n",
    "    if index == 0 and len(dp) == 1:\n",
    "        \n",
    "        prediction.append(sorted(dp[0], key=dp[0][1], reverse=True)[0])\n",
    "        print(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    elif index == 0:\n",
    "        prediction.append(from_)\n",
    "        return prediction[::-1]\n",
    "    \n",
    "    elif index == len(dp)-1 and from_ is None:\n",
    "        final_score = 0\n",
    "        for key, val in forward_dp[index].items():\n",
    "            cur_score = val['cur_best_score']\n",
    "            if cur_score > final_score:\n",
    "                final_score = cur_score\n",
    "                best_key = key\n",
    "                best_from = val['from']\n",
    "                cur_from_ = best_from\n",
    "                prediction.append(best_key)\n",
    "    else:\n",
    "        for key, val in dp[index].items():\n",
    "            if key == from_:\n",
    "                prediction.append(key)\n",
    "                cur_from_ = val['from']\n",
    "    return viterbi_backward(dp, index-1, cur_from_, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████▋                    | 2642/5526 [01:25<01:33, 30.76it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_y/p58ng_q55hz1jd4rzl4rzs1c0000gn/T/ipykernel_73894/2017943381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mforward_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_y/p58ng_q55hz1jd4rzl4rzs1c0000gn/T/ipykernel_73894/477779964.py\u001b[0m in \u001b[0;36mviterbi_backward\u001b[0;34m(dp, index, from_, prediction)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "p = []\n",
    "l = []\n",
    "for sentence in tqdm(dev_sentences):\n",
    "    dp = [{} for _ in range(len(sentence))]\n",
    "    forward_dp = viterbi_forward(sentence, dp)\n",
    "    predictions = viterbi_backward(forward_dp, len(forward_dp)-1 , None, [])\n",
    "    labels = [x[2] for x in sentence]\n",
    "    p = p + predictions\n",
    "    l = l + labels\n",
    "print(acc(p, l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
