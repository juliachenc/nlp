{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"dev\", sep = \"\\t\", index_col=False, header=None)\n",
    "dev.columns = [\"index\", \"word\", \"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train\", sep = \"\\t\", index_col=False, header=None)\n",
    "train.columns = [\"index\", \"word\", \"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test\", sep = \"\\t\", index_col=False, header=None)\n",
    "test.columns = [\"index\", \"word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique vocab\n",
    "countVoc = train['word'].value_counts().rename_axis('word').reset_index(name='counts')\n",
    "# less freq part\n",
    "lessfreq = countVoc.loc[countVoc['counts'] < 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the word as '<unk>' if the frequence less than 3 \n",
    "countVoc.loc[countVoc['counts'] < 3, 'word'] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the summation counts where the words are the same,\n",
    "# the only case where words are different is '<unk>'\n",
    "countVoc = countVoc.groupby(['word']).sum('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort thhe vocabulary by its count\n",
    "countVoc = countVoc.sort_values(by=['counts'], ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# move 'unk' to the first line\n",
    "unk_word = countVoc[countVoc['word']=='<unk>']\n",
    "countVoc = countVoc[countVoc['word']!='<unk>'].reset_index()\n",
    "vocab = pd.concat([unk_word, countVoc], ignore_index = True, axis = 0)\n",
    "vocab = vocab.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>32537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>46476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>39533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>37452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>22104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16915</th>\n",
       "      <td>overreacting</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16916</th>\n",
       "      <td>Schuster</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16917</th>\n",
       "      <td>overriding</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16918</th>\n",
       "      <td>oversaw</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16919</th>\n",
       "      <td>patience</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  counts\n",
       "0             <unk>   32537\n",
       "1                 ,   46476\n",
       "2               the   39533\n",
       "3                 .   37452\n",
       "4                of   22104\n",
       "...             ...     ...\n",
       "16915  overreacting       3\n",
       "16916      Schuster       3\n",
       "16917    overriding       3\n",
       "16918       oversaw       3\n",
       "16919      patience       3\n",
       "\n",
       "[16920 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pd.concat([unk_word, countVoc], ignore_index = True, axis = 0)\n",
    "vocab = vocab.drop('index', axis=1)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected threshold for unknown vocabulary is 3, so the size of my \n",
    "vocabulary is 13751, the occurrences of token '< unk >' after replacement are 42044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab .to_csv('/Users/juliachen/Desktop/CSCI 544/homework/hw3/vocab.txt',\n",
    "#              header = False,index=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pierre</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912090</th>\n",
       "      <td>22</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912091</th>\n",
       "      <td>23</td>\n",
       "      <td>san</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912092</th>\n",
       "      <td>24</td>\n",
       "      <td>francisco</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912093</th>\n",
       "      <td>25</td>\n",
       "      <td>instead</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912094</th>\n",
       "      <td>26</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>912095 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       word    tag\n",
       "0           1     pierre    NNP\n",
       "1           2      <unk>  <unk>\n",
       "2           3          ,      ,\n",
       "3           4         61     CD\n",
       "4           5      years    NNS\n",
       "...       ...        ...    ...\n",
       "912090     22         to     TO\n",
       "912091     23        san    NNP\n",
       "912092     24  francisco    NNP\n",
       "912093     25    instead     RB\n",
       "912094     26          .      .\n",
       "\n",
       "[912095 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train\n",
    "train2.loc[train2['word'].isin(lessfreq['word']), 'word'] = '<unk>'\n",
    "train2.loc[(train2['word'] == '<unk>'), 'tag'] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case\n",
    "train2\n",
    "train2['word'] = train2['word'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition parameters for HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find unique state as a dictionary\n",
    "2. Creat empty dictionary for count(s->s')\n",
    "3. For each sentence, go over all tags \n",
    "4. Calculate the transition by count(s->s')/count(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate s\n",
    "count_tags= train2['tag'].value_counts().rename_axis('tag').reset_index(name='count')\n",
    "tags = count_tags.set_index('tag').to_dict()['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate s->s'\n",
    "tags_tran = {}\n",
    "\n",
    "for key, val in tags.items():\n",
    "    tags_tran[key] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate next state s' for each s\n",
    "tag_list = train2['tag'].to_list()\n",
    "\n",
    "for i in range(len(tag_list)-1):\n",
    "    s = tag_list[i]\n",
    "    s_prime = tag_list[i+1]\n",
    "    tags_tran[s][s_prime] = tags_tran[s].get(s_prime, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition \n",
    "transition = {}\n",
    "\n",
    "for key, val in tags_tran.items():\n",
    "    count = tags[key]\n",
    "    for key2, val2 in val.items():\n",
    "        total = val2\n",
    "        transition[(key, key2)] = total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = dict((','.join(k), v) for k,v in transition.items())\n",
    "#with open('/Users/juliachen/Desktop/CSCI 544/homework/hw3/transition.json', 'w') as fp:\n",
    "#    json.dump(data, fp, indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission parameters for HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find unique state as a dictionary\n",
    "2. Creat empty dictionary for count(s->x)\n",
    "3. For each sentence, go over all word\n",
    "4. Calculate the transition by count(s->x)/count(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate s->x\n",
    "emm_tran = {}\n",
    "\n",
    "for key, val in tags.items():\n",
    "    emm_tran[key] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate next state x for each s\n",
    "word_list = train2['word'].to_list()\n",
    "tag_list = train2['tag'].to_list() \n",
    "\n",
    "for i in range(len(word_list)):\n",
    "    x = word_list[i]\n",
    "    s = tag_list[i]\n",
    "    emm_tran[s][x] = emm_tran[s].get(x, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# emission  \n",
    "emission = {}\n",
    "\n",
    "for key, val in emm_tran.items():\n",
    "    count = tags[key]\n",
    "    for key2, val2 in val.items():\n",
    "        total = val2\n",
    "        #print(key2, \":\", total)\n",
    "        emission[(key, key2)] = total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = dict((','.join(k), v) for k,v in emission.items())\n",
    "#with open('/Users/juliachen/Desktop/CSCI 544/homework/hw3/emission.json', 'w') as fp:\n",
    "#    json.dump(data, fp, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1445 transition parameters, and 21754 emission parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(transition), \"transition parameters, and\", len(emission), \"emission parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1451 transition parameters and 23340 emission parameters in my HMM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Greedy Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/juliachen/Desktop/CSCI 544/dev', 'r') as fp:\n",
    "    sentence = []\n",
    "    dev_sentences = []\n",
    "    for line in fp:\n",
    "        line = line.replace('\\n', '').split('\\t')\n",
    "        #print(len(line))\n",
    "        if len(line) == 1:\n",
    "            dev_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dev = len(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition(previous_tag):\n",
    "    tags_list = {}\n",
    "    for tag, val in transition.items():\n",
    "        if tag[0] == previous_tag:\n",
    "            #tags_list.append((tag[1], val))\n",
    "            tags_list.update({tag[1]: val})\n",
    "            \n",
    "    return tags_list\n",
    "\n",
    "# get_transition('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission(word):\n",
    "    tags_list ={}\n",
    "    if word in vocab['word'].to_list():\n",
    "        for tag, val in emission.items():\n",
    "            if tag[1] == word:\n",
    "                tags_list.update({tag[0]: val})\n",
    "    else: \n",
    "        for tag, val in emission.items():\n",
    "            if tag[1] == '<unk>':\n",
    "                tags_list.update({tag[0]: val})\n",
    "                \n",
    "    return tags_list\n",
    "# get_emission('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstSent = train2[train2['index']==1]\n",
    "ts1 = dict(firstSent['tag'].value_counts())\n",
    "for key, val in ts1.items():\n",
    "    ts1[key] = val/len(firstSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.unique(dev['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate s*1 = arg max t(s1)e(x1|s1)\n",
    "def s1(state):\n",
    "    score = {}\n",
    "    for tag, val in state.items():\n",
    "        if tag in ts1:\n",
    "            score[tag] = val*ts1[tag]\n",
    "        else: \n",
    "            continue\n",
    "    print(score)\n",
    "    return(max(score, key = score.get))\n",
    "\n",
    "# def s1(word):\n",
    "#     # prob_w = get_emission(word)\n",
    "#     bestScore = 0 \n",
    "#     for tag, prob in word:\n",
    "#         if tag in ts1:\n",
    "#             score = ts1[tag] * prob\n",
    "#         else: \n",
    "#             score = ts1['<unk>'] * prob\n",
    "#         if score > bestScore:\n",
    "#             bestScore = score\n",
    "#             ps1 = tag\n",
    "            \n",
    "#     return ps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DT': 4.8198754513579996, 'NN': 0.36351980742058715}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DT'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1({'DT': 22, 'NN': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s*2 = arg max t(s2|s*1)e(x2|s2)\n",
    "def s2(word, state):\n",
    "    score = {}\n",
    "    for key in state:\n",
    "        if key in word:\n",
    "            score[key] = word[key]*state[key]\n",
    "    return(max(score, key = score.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-08331d663533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"like\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'DT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-270-4383c081ae97>\u001b[0m in \u001b[0;36ms2\u001b[0;34m(word, state)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "s2(\"like\", {'DT': 22, 'NN': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(pred, label):\n",
    "    # assert(len(pred)==len(label))\n",
    "    return sum(x[0]==x[1] for x in zip(pred, label))/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greed_model(sentence):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for word in sentence:\n",
    "        index = word[0]\n",
    "        token = word[1]\n",
    "        tag = word[2]\n",
    "        labels.append(tag)\n",
    "        if index == '1':\n",
    "            tag_by_word = get_emission(token)\n",
    "            pred_tag = s1(tag_by_word)\n",
    "            predictions.append(pred_tag)\n",
    "            prev_tag = pred_tag\n",
    "        else:\n",
    "            try:\n",
    "                tag_by_word = get_emission(token)\n",
    "                tag_by_tag = get_transition(prev_tag)\n",
    "                pred_tag = s2(tag_by_word, tag_by_tag)\n",
    "                predictions.append(pred_tag)\n",
    "                prev_tag = pred_tag\n",
    "            except ValueError:\n",
    "                pred_tag = max(tag_by_word, key = tag_by_word.get)\n",
    "                predictions.append(pred_tag)\n",
    "                prev_tag = pred_tag\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-1f576aae384f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdev_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreed_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-af2f8971cc0e>\u001b[0m in \u001b[0;36mgreed_model\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtag_by_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mpred_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_by_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprev_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-269-026541c7efc9>\u001b[0m in \u001b[0;36ms1\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# def s1(word):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "for sentence in dev_sentences:\n",
    "    predictions, labels = greed_model(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-64b6e9e34c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "acc(predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition parameters for HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find unique state\n",
    "2. Build empty matrix\n",
    "3. Go over the index of each sentence, find count(s ->s')\n",
    "4. Count the dominodator, count(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.unique(train['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_S = train['tag'].value_counts().rename_axis('word').reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat = np.zeros([len(state), len(state)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = train['index'][0]\n",
    "last_tag = train['tag'][0]\n",
    "for i,j in zip(train['index'][1:], train['tag'][1:]):  \n",
    "    ni=np.where(state == last_tag)[0][0]\n",
    "    nj=np.where(state == j)[0][0]\n",
    "    if i == last_index + 1:\n",
    "        trans_mat[ni,nj] += 1\n",
    "    last_index = i\n",
    "    last_tag = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission parameters for HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find unique state and unique vocab \n",
    "2. Build empty matrix\n",
    "3. Go over the index of each sentence, find count(s ->x)\n",
    "4. Count the dominodator, count(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = np.unique(train['word'])\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emiss_mat = np.zeros([len(state), len(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = train['index'][0]\n",
    "last_tag = train['tag'][0]\n",
    "n_sentence = 0\n",
    "for i,j,k in zip(train['index'][1:], train['tag'][1:], train['word'][1:]):\n",
    "    ni=np.where(state == last_tag)[0][0]\n",
    "    nj=np.where(vocab == k)[0][0]\n",
    "    if i == last_index + 1:\n",
    "        emiss_mat[ni,nj] += 1\n",
    "    else:\n",
    "        n_sentence += 1\n",
    "        print(f'Complete one sentence {n_sentence}') \n",
    "    last_index = i\n",
    "    last_tag = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greed_model(sentence):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    # sentence_len = 0\n",
    "    for item in sentence:\n",
    "        sentence_len += 1\n",
    "        idx = item[0]\n",
    "        word = item[1]\n",
    "        label_tag = item[2]\n",
    "        labels.append(label_tag)\n",
    "\n",
    "        if idx == '1':\n",
    "            initial_word = word\n",
    "            potential_words = get_emission(initial_word)\n",
    "            ps1 = s1(potential_words)\n",
    "            previous_tag = ps1\n",
    "            predictions.append(ps1)\n",
    "            continue\n",
    "\n",
    "        potential_words2 = get_emission(word)\n",
    "        potential_tags = get_transition(previous_tag)\n",
    "        try: \n",
    "            print(word, tag)\n",
    "            ps2 = s2(word,tag)\n",
    "        except:\n",
    "            ps2 = sorted(potential_words2, key = lambda x: x[1], reverse = True)[0][0]\n",
    "        previous_tag = ps2\n",
    "        predictions.append(ps2)\n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greed_out(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greed_out(sentence):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    sentence_len = 0\n",
    "    for i in sentence:\n",
    "        sentence_len += 1\n",
    "        idx = i[0]\n",
    "        word = i[1]\n",
    "        label_tag = i[2]\n",
    "        labels.append(label_tag)\n",
    "        \n",
    "        if idx =='1':\n",
    "            initial_word = word\n",
    "            potential_words = get_potential_words(initial_word)\n",
    "            ps1 = get_ps1(potential_words)\n",
    "            previous_tag = ps1\n",
    "            prediction.append(ps1)\n",
    "            continue\n",
    "            \n",
    "        potential_words2 = get_potential_words(word)\n",
    "        potential_tags = get_potential_tags(previous_tag)\n",
    "        try: \n",
    "            ps2 = get_ps2(potential_words2, potential_tag)\n",
    "        except:\n",
    "            ps2 = sorted(potential_words2, key = lambda x: x[1], reverse = True)[0][0]\n",
    "        previous_tag = ps2\n",
    "        predictions.append(ps2)\n",
    "    return predictions\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
